{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankraj1234/MediGuide/blob/master/mediguide_qlora.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akR2C7IUhsjQ"
      },
      "source": [
        "**IMPORTS AND INSTALLATIONS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBqPee8vhh6C"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install accelerate peft bitsandbytes transformers trl evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnT_HF08hcTV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60PntG08h1JN"
      },
      "source": [
        "Upload the train dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yCRVqsBhzQI",
        "outputId": "896fd83f-a001-40da-a11d-2884dbef9f8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "file_path = '/content/sampled_6000.json'\n",
        "drive_path = \"/content/drive/MyDrive/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2bvF5OHu06T",
        "outputId": "41b04abb-8241-4524-972a-55ab7d10a94d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully loaded 6000 medical examples\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "try:\n",
        "    with open(file_path) as f:\n",
        "        try:\n",
        "            medical_data = json.load(f)\n",
        "        except json.JSONDecodeError:\n",
        "\n",
        "            content = f.read().split('[file content end]')[0].split('[file content begin]')[-1].strip()\n",
        "            medical_data = json.loads(content)\n",
        "\n",
        "    print(f\"Successfully loaded {len(medical_data)} medical examples\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {file_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zcc4wOnph-A8",
        "outputId": "caf60ad7-63d7-40a2-b784-710384e70f5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sample data item:\n",
            "Instruction: If you are a doctor, please answer the medical questions based on the patient's description.\n",
            "Input: I wake in the night, usually about 2-3 hours after...\n",
            "Output: Dear patient Here are the possibilities of what yo...\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nSample data item:\")\n",
        "print(f\"Instruction: {medical_data[0]['instruction']}\")\n",
        "print(f\"Input: {medical_data[0]['input'][:50]}...\")\n",
        "print(f\"Output: {medical_data[0]['output'][:50]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWvHy5IqiI9_"
      },
      "outputs": [],
      "source": [
        "# Data format for Mistral model\n",
        "def format_data(sample):\n",
        "        return {\n",
        "            \"text\": f\"[MED] {sample['instruction']}\\nPatient: {sample['input']}\\nDoctor: {sample['output']}\"\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vp-xSAPAiRDc"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "dataset = [format_data(d) for d in medical_data]\n",
        "dataset = Dataset.from_list(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAGluyYAiVHo"
      },
      "outputs": [],
      "source": [
        "# define an output directory to save partially trained model so as to resume later\n",
        "output_dir = \"/content/drive/MyDrive/medical_qlora\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g414G7SZieEh"
      },
      "outputs": [],
      "source": [
        "def find_latest_checkpoint(output_dir):\n",
        "    try:\n",
        "        if not os.path.exists(output_dir):\n",
        "            print(f\"Output directory {output_dir} does not exist\")\n",
        "            return None\n",
        "\n",
        "        if not os.listdir(output_dir):\n",
        "            print(f\"Output directory {output_dir} is empty\")\n",
        "            return None\n",
        "\n",
        "        checkpoints = [d for d in os.listdir(output_dir)\n",
        "                      if d.startswith(\"checkpoint\") and os.path.isdir(os.path.join(output_dir, d))]\n",
        "\n",
        "        if not checkpoints:\n",
        "            print(\"No checkpoint directories found\")\n",
        "            return None\n",
        "\n",
        "        checkpoints.sort(key=lambda x: int(x.split(\"-\")[1]))\n",
        "        latest = os.path.join(output_dir, checkpoints[-1])\n",
        "        print(f\"Found checkpoint: {latest}\")\n",
        "        return latest\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error finding checkpoint: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDsNLSAJilV3",
        "outputId": "f71564d7-b395-4253-e187-ded82c6aedc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found checkpoint: /content/drive/MyDrive/medical_qlora/checkpoint-375\n",
            "Latest checkpoint: /content/drive/MyDrive/medical_qlora/checkpoint-375\n"
          ]
        }
      ],
      "source": [
        "latest_checkpoint = find_latest_checkpoint(output_dir)\n",
        "print(f\"Latest checkpoint: {latest_checkpoint}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbNTL-_fjSIz"
      },
      "source": [
        "Setting up model and hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkQ0FxzMioL4"
      },
      "outputs": [],
      "source": [
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "new_model = \"mistral-chat-finetune\"\n",
        "\n",
        "# Qlora parameters\n",
        "\n",
        "lora_r = 64\n",
        "lora_alpha = 16\n",
        "lora_dropout = 0.1\n",
        "\n",
        "\n",
        "use_4bit = True\n",
        "bnb_4bit_compute_dtype = \"float16\"\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "use_nested_quant = False\n",
        "num_train_epochs = 1\n",
        "\n",
        "fp16 = True\n",
        "bf16 = False\n",
        "\n",
        "per_device_train_batch_size = 4\n",
        "gradient_accumulation_steps = 4\n",
        "gradient_checkpointing = True\n",
        "max_grad_norm = 0.3\n",
        "\n",
        "\n",
        "learning_rate = 2e-4\n",
        "weight_decay = 0.001\n",
        "optim = \"paged_adamw_8bit\"\n",
        "lr_scheduler_type = \"cosine\"\n",
        "\n",
        "max_steps = -1\n",
        "\n",
        "\n",
        "warmup_ratio = 0.03\n",
        "group_by_length = True\n",
        "save_strategy=\"steps\",\n",
        "save_steps=200,\n",
        "save_total_limit=3,\n",
        "\n",
        "\n",
        "logging_steps = 25\n",
        "max_seq_length = 1024\n",
        "packing = False\n",
        "\n",
        "\n",
        "device_map = {\"\": 0}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401,
          "referenced_widgets": [
            "7fbf8117329b48d088a5a127fe9ba265",
            "7e9c940381a94ca294978b9e58efe5bc",
            "b83d8a6a7ba74e2da0cdab8eddf10c09",
            "95c60109cbba4054beed247299248708",
            "a91e3906a2854fb286638b127efeea25",
            "bc784e35dab34c5e83c2c95dbaa9ec3f",
            "955742fd7a8f4ec38adb9603ace991b1",
            "49576d6cf00947a0ad9b8bc09e1933b2",
            "5d928be2e98e43be89127e328d7e9d31",
            "554117a50d5f40b8bac584f339d8266b",
            "cc1225f059ca43fc955a789989d2477a",
            "d7bf21b618d54822ac01aa12779656a9",
            "9307671d50f742719182cbbe89e33f03",
            "7aff9016afc542eebfda7a58ad3d7ba3",
            "2de82612623c4479a709554ee3392d68",
            "ccd7e43bfdba4ca3b0039fdc2d861aee",
            "5f6538f682a149ecbb216d6a560d91a3",
            "e778611423fb4ed388ca8f16a986e226",
            "03639c61a3074271a2438389b21503d1",
            "4eb0384caff44ea7a56077fe56202039",
            "38d58d70cc0a4c8daa5353a07927ec36",
            "673d662ce62246d78a6ed3093d46a98a",
            "ab551522e92549948ec849bffb2640a3",
            "65c124d3be6e45b192e846571738be95",
            "b6e752ed8197473fbc52ef5fb27f2557",
            "8251f05424a841f8923576ea3132b20a",
            "bb177e32cf264dec8320e4019114b69a",
            "6d958b77bc7041d4b52a0a67fd7778e8",
            "dbe44b65f76e4865a943c1ca2b0b5687",
            "e6bf2355b60e4b4cb164b58c1f4e6a32",
            "6af95801e9eb43559221e1810514b0c6",
            "55959c07dba948bc9c24292a01f0b64d",
            "1fb956dfa0a34b5993193983f06570a2",
            "a35f891fa64d4122a4bf3d2560ce9647",
            "27d65f9976124b19bbfc26ed84b4a4ba",
            "7d9036d0873746f2ac11faf2b4905b80",
            "f7d6c36eb5c1468f9a4cc5731d72d4c1",
            "23772628647544ff9cbd0cbd28d9a8c2",
            "e8d7c6a58ffb4131886f3c10acd9454a",
            "16b9e02e7e80428b84c3f526d40732ae",
            "fc4a910c99fb493db6b0c5041e2442d9",
            "fdb19cfd235f4b43863dca629b296946",
            "0051f2f4e7d6410282315b831206869f",
            "20a4dfd6a29b47aeb2a2f769ba72dac7",
            "aee26a37952b45cba2e82ebbf0b5d028",
            "4805f82b00374c3e8cf82313730a1748",
            "0bf2e998cbe143359ea09f25202480fa",
            "10d028414262408b928ff268489cfbd8",
            "98c6271b18c74794ac9d1188fe7da36a",
            "fd6bb37cb27b440aae46a96ff671242b",
            "b2d77aedca1a4bef9947e8e1e62ad8f6",
            "b82d7d59b244430993d5bad16362b539",
            "e0037a57842c455eafa5679f5e3a812d",
            "1029279bbf8241e0b9ea7aa0c02e1a00",
            "347a43f988fa4dd5b94bc9cd47706b4b",
            "ee2a802cb6aa41b78eebeaa2d48f1c56",
            "273a97c491514ae39a57675e1400712a",
            "d91c7c109c834553af8597d49f28e769",
            "9e4287d7bdec4a1bacc8cdc1acfcd8db",
            "fb4dc670cf7b4de9b83af10b0abd78a8",
            "8e2b9040b2e54e7c95d3d50b10043805",
            "09c580c41f404250b27d6ff666586de0",
            "2879310ddda24a47bd111ca5db916e0e",
            "20e9fc3eb9644b8090f22f997455bdc5",
            "a3c19946d3c44dd0bb393c7d08558e01",
            "4027e2da0c534298b72f33dabe7964f9",
            "c6918d38971f46e1b8f9887057616307",
            "4b75da8b34394ea0bfd87769345eb584",
            "18dadec45c544b48a2c3eaca4014a3ca",
            "9607c52e218f43a6947cdca0e44971c5",
            "0befe0523caa41b5b4161dbd8fb75490",
            "30f38648806f4748b6231ee2280161c1",
            "7d9a7e544a054bc6a9c585c64483e352",
            "95eff0938cc64b4084d71978577d17f8",
            "632e4b341e184fc9ab69cacfdb184977",
            "41564d5a07484db1984f7b578eec986e",
            "db56afcc6c1d4793993aa4ea3840301b",
            "6f8de431029c4ab19dcc26865c8191cc",
            "f25becd7d19b49379093d2f80e241814",
            "0b4c2bef7869468fb149e0f088149552",
            "74a6ae5069884fd6b4384025ba981cb5",
            "47dd26979ba744a688c0ebce80b592c1",
            "c15f1fc6b7d54a42accf8817ea4ef1b1",
            "93e2761b5abc4c80885207bffbbae3f5",
            "2bdd06948dee4eff9eded49858278ed8",
            "b4cd883439c943ad8e8d688ed2d81392",
            "58b997f5d274476b806a8748f0ee14eb",
            "8ed2a2050ec546bd9ea16b7531308232",
            "58c3afaa5da4472ab980d171499a06c1",
            "61e4133bbc04427aa9123fbbd8986edb",
            "b16f54432b0b4f3c972b17da2e80b675",
            "293ac527f1a04829b7c4bc2bf74224ec",
            "0c046d0e261f4cb6adec2023415a796f",
            "25357a5b25e040af9f606c1e248e0a26",
            "edfec06aa5fb4890b08fd233e7a610f6",
            "25d087e9561245fcbf1ca9092238ec05",
            "ad0e0bec4aa143e098427bb12ae11268",
            "744b3cf5bc734ddf87b06599496d655b",
            "43a268998d5e4df5a645b63d6c12513e",
            "89ea5091ef364d4cb292d9a9f1f8f79d",
            "5ab9952e202549ef8d84ff4d8fa08337",
            "23735fb31aaa4356a6a308385ace0f06",
            "246c9223de3149a4b2a72ff99fcc9259",
            "5b3bfee638ab417c971e5cac5b35389c",
            "a7461aae8ae04dc5adb8536e7b4f4057",
            "29d9883c766c45428b0907e63e29416b",
            "88be329e49414b5da246e36040f1650e",
            "edd55368455e4f1f939e91b60dc891d2",
            "22cb17054ebb4eb0a5c2eb6c10830517",
            "ec0cf5699585442db076c31e43c58087",
            "76a2b2d4abd34bfb8e817b19d1a81c47",
            "d2f7db9ecff746b894e7425382aaeaad",
            "71dd94295ec149c29ef1f6d7bf60326f",
            "1448c5f63d7a46c4ba5b8507125df7d4",
            "f58ceaa57d01491cae512c4a28432749",
            "bbf8bc81172146379bed550900c89c00",
            "797bcab0ea37470e9cd360b6e23163c2",
            "35240372424143b9b0aa374dafd73b2d",
            "27cc33809e5142c188a885a852be2c61",
            "a653068ffe0444c186c7e5a1d09aa030",
            "7b5c407f5c9f4515a61205cafe570244",
            "b7d12fce856b4647a2e7972dac12660e",
            "71251860be3249e1a1d6385fdce3ea57",
            "ca65c6d0b8ca4957a1f49e25d16ba3c0",
            "0fff464b9ab745a6a33449fcb2ed60b4",
            "d9381f7e377a4c358c01ef3fb6613177",
            "f7321d161ffb48838a633129475dd5d8",
            "b8e89afd3c4a4bc8b1c4d8377967e8ea",
            "6693a977ae6048c094b296fa5869864b",
            "67804d551fde48f1aed76cbb7fc21b4e",
            "3700bba8a5564ab192f40b70635ea189",
            "d335cc541e764dde9d290e7bd44d9a0e"
          ]
        },
        "id": "1Z8X1P05jbSg",
        "outputId": "c42428da-b999-45ec-9115-1149c1bfa3df"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7fbf8117329b48d088a5a127fe9ba265",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/601 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d7bf21b618d54822ac01aa12779656a9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ab551522e92549948ec849bffb2640a3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a35f891fa64d4122a4bf3d2560ce9647",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aee26a37952b45cba2e82ebbf0b5d028",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00003-of-00003.safetensors:   0%|          | 0.00/4.55G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ee2a802cb6aa41b78eebeaa2d48f1c56",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c6918d38971f46e1b8f9887057616307",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6f8de431029c4ab19dcc26865c8191cc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "58c3afaa5da4472ab980d171499a06c1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/141k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "89ea5091ef364d4cb292d9a9f1f8f79d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "76a2b2d4abd34bfb8e817b19d1a81c47",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b7d12fce856b4647a2e7972dac12660e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load tokenizer and model with QLoRA configuration\n",
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")\n",
        "\n",
        "# Load base model (if training for the first time)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=device_map\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# Load LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "\n",
        "# Convert any tuple values to single integers/floats\n",
        "if isinstance(save_steps, tuple):\n",
        "    save_steps = save_steps[0]\n",
        "if isinstance(logging_steps, tuple):\n",
        "    logging_steps = logging_steps[0]\n",
        "if isinstance(max_steps, tuple):\n",
        "    max_steps = max_steps[0]\n",
        "if isinstance(save_total_limit, tuple):\n",
        "    save_total_limit = save_total_limit[0]\n",
        "if isinstance(num_train_epochs, tuple):\n",
        "    num_train_epochs = num_train_epochs[0]\n",
        "if isinstance(per_device_train_batch_size, tuple):\n",
        "    per_device_train_batch_size = per_device_train_batch_size[0]\n",
        "if isinstance(gradient_accumulation_steps, tuple):\n",
        "    gradient_accumulation_steps = gradient_accumulation_steps[0]\n",
        "if isinstance(learning_rate, tuple):\n",
        "    learning_rate = learning_rate[0]\n",
        "if isinstance(weight_decay, tuple):\n",
        "    weight_decay = weight_decay[0]\n",
        "if isinstance(max_grad_norm, tuple):\n",
        "    max_grad_norm = max_grad_norm[0]\n",
        "if isinstance(warmup_ratio, tuple):\n",
        "    warmup_ratio = warmup_ratio[0]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caXwc4jVltJw",
        "outputId": "ce3d2b5d-1bc5-4283-e38a-c224824d77bb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'mistralai/Mistral-7B-Instruct-v0.3' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/peft_model.py:569: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.base_model.model.base_model.model.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.base_model.model.base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight'].\n",
            "  warnings.warn(warn_message)\n"
          ]
        }
      ],
      "source": [
        "# If continuing training from some checkpoint\n",
        "from peft import get_peft_model, PeftModel\n",
        "model = get_peft_model(model, peft_config)\n",
        "model = PeftModel.from_pretrained(\n",
        "    model,\n",
        "    output_dir,\n",
        "    revision=latest_checkpoint.split(\"/\")[-1],\n",
        "    is_trainable=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfZOcaNEjrb0"
      },
      "outputs": [],
      "source": [
        "# Set training parameters\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=save_steps,\n",
        "    save_total_limit=save_total_limit,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    fp16=fp16,\n",
        "    bf16=bf16,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=group_by_length,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    report_to=\"tensorboard\",\n",
        "    resume_from_checkpoint = latest_checkpoint,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mqdlx6QkDAc"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "        return tokenizer(\n",
        "            examples[\"text\"],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=256,\n",
        "            return_tensors=None\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "87b40d2c20f147dfaa87b873a6b1d877",
            "5f3765a9eeb444bb99eeab3fce19fe02",
            "817aff6ae3804cc98dc1cbb8a96e84ba",
            "91458862390c4837bba5f6060195d200",
            "f2194cffac874c8490feb8b2bde64a6e",
            "4698e496dcad47018d00675a89e10ade",
            "b35587b95d374e0e9049075031e7f8f8",
            "8af14c8944c64079927975a440e1d3bc",
            "fde9d6fb872e40278bfb5993c7e27e26",
            "f7a1cd0cd4f94172b7d5391b300bb87c",
            "0bc52aee26164306a6ccb01c9f239795",
            "9b4468a100bb4973bc9b2a643615bc67",
            "0bae9b0f36804efc821476cf509389fe",
            "58c204ff77ef4e1b8fc280d8a6d2798a",
            "0d6d5bd4ca0f4edc93f5145630d88aea",
            "de11e551c34f48d59471fbe3322a4b23",
            "54b3df743f1840d0acb2eda777abf1d7",
            "0ec7df95910a44b097da9be0a092273e",
            "116edd500c7344178a0ce010fccade24",
            "2a75d72661ac4ec7988c8175e4aab3b8",
            "a9cfd5b2c83a4e648422b4d2493bd720",
            "972fcc3f1c5a4f1083acae04c4ffc69e"
          ]
        },
        "id": "haTsNRUzkDqn",
        "outputId": "49465e61-263e-4103-951d-760a170c6044"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "87b40d2c20f147dfaa87b873a6b1d877",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/6000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9b4468a100bb4973bc9b2a643615bc67",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/6000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "tokenized_dataset = tokenized_dataset.map(\n",
        "        lambda examples: {\"labels\": examples[\"input_ids\"].copy()},\n",
        "        batched=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjGSglH1wQTD"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=False\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87,
          "referenced_widgets": [
            "bc2a7f5972bf41f49d1455cecfd4b5f5",
            "a9d7d43a34084414b453e598e27574c5",
            "5fcdf1f119d34a968376b078b9747c3d",
            "160d594648c946f6b92606e6a512f9ba",
            "bcdfafa277d04979a56128796048f5ee",
            "64b105648aae4285ba598c1d2601507b",
            "616a4667984e43feab2d66cc81608d7f",
            "e9af84a9488340ce8a1533e5c98a56a2",
            "eaf724b180dc4f73907ba6535b410339",
            "4cf18e836b4b4e548d4f6e72c6df6ae5",
            "0ff5cbe0b6424a558d59146151367764"
          ]
        },
        "id": "UujBQtjAkzYG",
        "outputId": "98e799e9-a728-4ef6-8946-21412c68cca8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bc2a7f5972bf41f49d1455cecfd4b5f5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Truncating train dataset:   0%|          | 0/6000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        }
      ],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    peft_config=peft_config,\n",
        "    args=training_arguments,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    data_collator=data_collator\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114
        },
        "id": "TrXQaFs-lq-c",
        "outputId": "f4205856-c281-4c66-a354-9f0e16eb14a1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [375/375 : < :, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=375, training_loss=0.0, metrics={'train_runtime': 0.0352, 'train_samples_per_second': 170621.54, 'train_steps_per_second': 10663.846, 'total_flos': 6.58120900608e+16, 'train_loss': 0.0})"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train(resume_from_checkpoint=latest_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ro-Ycjtal5lX"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "trainer.model.save_pretrained(output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0eiX7q6mSof"
      },
      "source": [
        "Pushing the fully trained model to Huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0jsPgrmmsBe"
      },
      "outputs": [],
      "source": [
        "# Enter hf access token here\n",
        "from huggingface_hub import login\n",
        "login(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXNVZy28mQiZ"
      },
      "outputs": [],
      "source": [
        "merged_model = trainer.model.merge_and_unload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKrE9an2mb2U"
      },
      "outputs": [],
      "source": [
        "merged_dir = \"merged_mistral_mediguide\"\n",
        "merged_model.save_pretrained(merged_dir)\n",
        "tokenizer.save_pretrained(merged_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzpWIueCmiqP"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import HfApi, HfFolder, create_repo, notebook_login\n",
        "repo_name = \"Greyitis/mediguide_new\"\n",
        "create_repo(repo_name, repo_type=\"model\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBsMzMGymk8W"
      },
      "outputs": [],
      "source": [
        "merged_model.push_to_hub(repo_name)\n",
        "tokenizer.push_to_hub(repo_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZ8D__6ym6sQ"
      },
      "outputs": [],
      "source": [
        "# To push complete fine tuned model instead of lora apdapters on top of model\n",
        "\n",
        "base = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=device_map\n",
        ")\n",
        "peft_model = PeftModel.from_pretrained(base, output_dir, is_trainable=False)\n",
        "\n",
        "# 2) Merge LoRA adapters into the base weights\n",
        "merged_model = peft_model.merge_and_unload()\n",
        "\n",
        "# 3) Save merged model & tokenizer\n",
        "merged_dir = \"merged_mediguide\"\n",
        "os.makedirs(merged_dir, exist_ok=True)\n",
        "merged_model.save_pretrained(merged_dir)\n",
        "tokenizer.save_pretrained(merged_dir)\n",
        "\n",
        "\n",
        "\n",
        "token = HfFolder.get_token()\n",
        "repo_id = \"Greyitis/mediguide_new\"\n",
        "create_repo(repo_id, repo_type=\"model\", exist_ok=True)\n",
        "\n",
        "# Upload the entire local directory\n",
        "api = HfApi()\n",
        "api.upload_folder(\n",
        "    folder_path=\"merged_mistral_mediguide\",\n",
        "    repo_id=repo_id,\n",
        "    repo_type=\"model\",\n",
        "    path_in_repo=\"\",\n",
        "    token=token,\n",
        ")\n",
        "\n",
        "print(\"Everything from merged_mistral_mediguide/ is now in\", repo_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOcTy1PcoSOt"
      },
      "source": [
        "**INFERENCING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PT6ZENIEoPoi"
      },
      "outputs": [],
      "source": [
        "# Custom stopper to prevent end answer generated at end of useful content\n",
        "from transformers import StoppingCriteria, StoppingCriteriaList\n",
        "\n",
        "class StopOnThanks(StoppingCriteria):\n",
        "    def __init__(self, tokenizer, stop_str=\"Regards\"):\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.stop_str = stop_str\n",
        "\n",
        "    def __call__(self, input_ids, scores, **kwargs):\n",
        "        text = self.tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
        "        return text.rstrip().endswith(self.stop_str)\n",
        "\n",
        "def medical_response(patient_input):\n",
        "    prompt = f\"\"\"<|user|>\n",
        "As a doctor, answer medical questions\n",
        "Patient: {patient_input}<|end|>\n",
        "<|assistant|>\n",
        "\"\"\"\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        return_attention_mask=False\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    stop_criteria = StoppingCriteriaList([StopOnThanks(tokenizer)])\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=200,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        repetition_penalty=1.1,\n",
        "        no_repeat_ngram_size=3,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        stopping_criteria=stop_criteria,\n",
        "    )\n",
        "\n",
        "\n",
        "    gen = outputs[0][ inputs[\"input_ids\"].shape[-1] : ]\n",
        "    answer = tokenizer.decode(gen, skip_special_tokens=True).strip()\n",
        "    if \"Thanks\" in answer:\n",
        "        answer = answer[: answer.rfind(\"Thanks\") + len(\"Thanks\") ]\n",
        "    return answer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9T7jU0ooseJ",
        "outputId": "8e6d73d5-f086-46a8-ffb0-8867a8794514"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I'm not a doctor but I can try to help. Your symptoms could be indicative of a viral infection such as the flu or a cold. However, it's important to get professional medical advice for accurate diagnosis and treatment. If you are concerned about your health, please contact a healthcare provider immediately.\n",
            "\n",
            "Remember to rest, stay hydrated, and take over-the-counter pain relievers to manage your symptoms until you can see a doctor. Avoid spreading germs by washing your hands regularly and covering your mouth when coughing or sneezing.\n"
          ]
        }
      ],
      "source": [
        "print(medical_response(\"I have fever and body aches\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwdJDbBGoud_",
        "outputId": "8bbc7d6f-b57a-46c7-c44f-73eabc1042e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Given the symptoms of fatigue (tiredness), weight gain without apparent cause, and Cold Intolerance (sensitivity to cold temperatures), there are several potential conditions that could be causing these symptoms. Here are some possible differential diagnosis options:\n",
            "\n",
            "1. Hypothyroidism - Underactive thyroid gland leading to decreased metabolism, fatigue and weight gain.\n",
            "2. Type 2 Diabetes Mellitus - Impaired insulin production or resistance leading to high blood sugar levels, fatique, weight change and cold sensitivity.\n",
            "3. Cushing's Syndrome - Overproduction of cortisol hormone, resulting in fatigue weight gain and cold tolerance.\n",
            "4. Anemia - Low red blood cell count can cause fatigue.\n",
            "5. Chronic Fatigue Syndromes - A complex disorder characterized by prolonged fatigue not relieved by rest.\n",
            "6. Depression - Can present with\n"
          ]
        }
      ],
      "source": [
        "print(medical_response(\"A 45-year-old male presents with fatigue, weight gain, and cold intolerance. What are the possible differential diagnoses, and which lab tests would you recommend?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmzrkVlNozFE"
      },
      "source": [
        "**Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "614d555535f24ea2ad093c4bda3264e1",
            "b22736d4fd7049a4bee79b681adad14e",
            "2338c44f5a0f4f7ebc458c3a9bd6d583",
            "5997fed8e9c54eb79fd9818e5cdc3a80",
            "cc979c0037a147428529a119aa3fcb67",
            "efcb44a9306e4f53a1178e5223177124",
            "ad7bf212f96a4bf49041f330233b45bd",
            "64c4b04546a446d886c7208cc245cf3d",
            "c9564a773b504a37a51d9a2cf91155d5",
            "b2693c1e25314d4d943591d4643073db",
            "e092c7c04a924e3e9f5772e5db44d77d",
            "c5228f05364d4ff6bae593a383de9891",
            "2b6e9c156d234926b45d59880f7fdad8",
            "ff00c2e9db744fdbaaf2e64fe2b11baa",
            "77d0bdf73abf4b7c913d45776b16ce19",
            "cf0d6816c0cf4d40a0a84a85371cdfa3",
            "afa71d0cf33a46d8a7461ca74e7c7adf",
            "c6b7553f67944064b02de54b67cf0588",
            "a2fe79e1cabd4e9a86c328943fd1601f",
            "46e098b0eb3f432bbd06be9eda17bf70",
            "5a078065a6944023a988a6dcad4fa10c",
            "7682833ae71f4cca8b0d5848d1b11bf4",
            "d847bce1dbfc411090489eb9ed19b0ea",
            "98e2a598a9e94c5ea577addb2b414dcf",
            "54da94bf6735485497ab9ff6e2ee2428",
            "f933783354ee40679a62b04d81388075",
            "44134d5bbf094f2cac3f65aad5153831",
            "b903e3cf61a0400484bc2c4003f45647",
            "0c37bf7a8de84d06a463410e7f76cb49",
            "355b99cd8de24819b6e76aef1db57a9c",
            "aa7e3b36d1c8415eb9b754325b32d284",
            "1beb28e38b124c99a0c0a449f9248295",
            "d757bed5f3ef4e13bedc5dfe34b7ed4f",
            "0b6525b8d02d477d9b67415a29d511ba",
            "e1829d1a5df14acb8b2512f1af04ebe4",
            "4d4d8de9b74449329a2af1b3752df3de",
            "954c250f8cfe40299596a912ad0e87c3",
            "cff369bbada44391b58b162cba09f515",
            "e39e58b4aac24734b8665eb6116f6187",
            "f5805871f9fc408587f6eb76270bc4bd",
            "b80566f340f64c21a6e3300426d2d5b7",
            "8e5b6e871e914b54b78bd3854c2d1c61",
            "317293b7e8924f1e99487389785fbd3e",
            "32af73a0257d48608bf8f64683d92c50",
            "c4fd101e5bac48aba82d999ebd218944",
            "7052db048c7943308668a0540de29afb",
            "aec6613d671f49bd86336c3165b6ad73",
            "b8aa85ce131d4ed7b3292c67d449a059",
            "a3b2fd08ab2044b8b11e58120396289a",
            "9daa272aeb48444097c14e07635189eb",
            "3b41e015a9a4416899d5ca77b5af018f",
            "16299ea27a9f40be9e69e4010af1607b",
            "4fb2dc1cb0b048509aa68fa9cf9cb8da",
            "18f8b5cdbe5c433f808dc8b94a85e605",
            "8bf2dc8e6edd4f70ac4b9b96636d7325"
          ]
        },
        "id": "dW56G-WXowoB",
        "outputId": "d862a839-cd6e-4245-f426-35b4f3f3724f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "614d555535f24ea2ad093c4bda3264e1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/965 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c5228f05364d4ff6bae593a383de9891",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "medicare_110k_train.json:   0%|          | 0.00/126M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d847bce1dbfc411090489eb9ed19b0ea",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "medicare_110k_test.json:   0%|          | 0.00/6.60M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0b6525b8d02d477d9b67415a29d511ba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/106556 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c4fd101e5bac48aba82d999ebd218944",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split:   0%|          | 0/5609 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "raw_datasets = load_dataset(\"Mohammed-Altaf/medical-instruction-120k\")\n",
        "test_split   = raw_datasets[\"test\"].select(range(1000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itADOsnTo4ux",
        "outputId": "ecc5e489-4668-4a7e-c551-ab731e78eddd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['Conversation'],\n",
            "    num_rows: 1000\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print(test_split)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "033bcd3c6aa54ceaa4c5fa0067ae5743",
            "c0a9fb724942487e9214621e453de75a",
            "15553a6ab6ce4ef2ad6eba2294054666",
            "4c9ef016b29848c4bb3ea9433bb169ab",
            "d23c8c652ed9486586e38dbc14d67eb7",
            "ccad9bcfb007423ab70edf0d962bb195",
            "9efb24e1ac6a483ebaf44540cf927ccf",
            "8310c0fb30734cbe881ea5f2587df814",
            "f0bc615993ac455db0e0e673e9642b0b",
            "8a99456e7f5740ed84dc973f8d3b6d14",
            "6f6f5866377f453ba8fe76f0192595d1"
          ]
        },
        "id": "8trYI6GMo6nF",
        "outputId": "d1c4e38d-976d-424a-d542-fe780f0ad0c5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "033bcd3c6aa54ceaa4c5fa0067ae5743",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "raw_datasets = load_dataset(\"Mohammed-Altaf/medical-instruction-120k\")\n",
        "test_split   = raw_datasets[\"test\"].select(range(1000))\n",
        "\n",
        "import re\n",
        "def extract_prompt_response(example):\n",
        "    \"\"\"\n",
        "    Parses the single-string conversation field into:\n",
        "      - instruction: text between [|Human|] and [|AI|] (or full text if no markers).\n",
        "      - response: text after the last [|AI|] marker (or empty if none).\n",
        "    \"\"\"\n",
        "    convo = str(example[next(iter(example.keys()))]).strip()\n",
        "    human_match = re.search(r\"\\[\\|Human\\|\\]\\s*(.*?)\\s*(?=\\[\\|AI\\|\\])\", convo, re.DOTALL)\n",
        "    instruction = human_match.group(1).strip() if human_match else convo\n",
        "    parts = re.split(r\"\\[\\|AI\\|\\]\", convo)\n",
        "    response = parts[-1].strip() if len(parts) > 1 else \"\"\n",
        "    return {\"instruction\": instruction, \"response\": response}\n",
        "\n",
        "test_df = test_split.map(\n",
        "    extract_prompt_response,\n",
        "    remove_columns=test_split.column_names,\n",
        "    num_proc=4\n",
        ")\n",
        "test_prompts = test_df[\"instruction\"]\n",
        "test_references = test_df[\"response\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "V1saDU_ko91V",
        "outputId": "4ed39e30-13f3-43d5-8bea-eda9ec4befaf"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'I wake in the night, usually about 2-3 hours after going to sleep, with both feet and legs to mid calf feeling like they are on fire. slight red discolorization, minor swelling. This is very painful but after getting up, I can walk it off in about 30 minutes.'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_prompts[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "_x-a90YSo_FB",
        "outputId": "e3cbcad2-91c5-4bb4-b6a9-56b7e6c84824"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Dear patient Here are the possibilities of what you might have.1)PhlebitisPhlebitis means inflammation of the veins, and can cause redness, itching, irritation, pain, and swelling. A simple Doppler can rule this out.2Blood clot in the lifeblood clots in the leg can become very dangerous, symptoms include swelling, redness, tenderness in the leg. Coagulation profile with an angiography may be required3)Cellulitis: Initial stage. Only can be clinically ruled out Hope this helped'"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_references[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jb9zSl2lpDYi"
      },
      "outputs": [],
      "source": [
        "# 1. PERPLEXITY\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "\n",
        "checkpoint_dir = \"/content/drive/MyDrive/medical_qlora\"\n",
        "eval_ckpt_path = \"/content/drive/MyDrive/medical_qlora/test_eval_state.pth\"\n",
        "batch_size = 2\n",
        "save_every_n_batches = 50\n",
        "\n",
        "\n",
        "class LMTestDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, texts, tokenizer, max_length=1024):\n",
        "        encodings = tokenizer(\n",
        "            texts,\n",
        "            return_tensors=\"pt\",\n",
        "            max_length=max_length,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\"\n",
        "        )\n",
        "        self.input_ids = encodings[\"input_ids\"]\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.input_ids.size(0)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"input_ids\": self.input_ids[idx],\n",
        "            \"labels\": self.input_ids[idx].clone()\n",
        "        }\n",
        "\n",
        "\n",
        "test_texts = [\n",
        "    f\"{instr}\\n\\n{resp}\"\n",
        "    for instr, resp in zip(test_prompts, test_references)\n",
        "]\n",
        "lm_test_dataset = LMTestDataset(test_texts, tokenizer, max_length=1024)\n",
        "\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    lm_test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator\n",
        ")\n",
        "\n",
        "device = model.device\n",
        "\n",
        "\n",
        "if os.path.isfile(eval_ckpt_path):\n",
        "\n",
        "    state = torch.load(eval_ckpt_path)\n",
        "    start_batch = state[\"last_batch\"] + 1\n",
        "    accumulated_loss = state[\"accumulated_loss\"]\n",
        "    total_tokens = state[\"total_tokens\"]\n",
        "    print(f\"Resuming test-eval from batch {start_batch} (saved on disk).\")\n",
        "else:\n",
        "\n",
        "    start_batch = 0\n",
        "    accumulated_loss = 0.0\n",
        "    total_tokens = 0\n",
        "    print(\"Starting test-eval from batch 0.\")\n",
        "\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for batch_idx, batch in enumerate(test_loader):\n",
        "        if batch_idx < start_batch:\n",
        "            continue\n",
        "\n",
        "\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            labels=labels\n",
        "        )\n",
        "        loss = outputs.loss.detach().cpu().item()\n",
        "\n",
        "\n",
        "        nonpad_tokens = (labels != tokenizer.pad_token_id).sum().item()\n",
        "\n",
        "\n",
        "        accumulated_loss += loss * nonpad_tokens\n",
        "        total_tokens += nonpad_tokens\n",
        "\n",
        "\n",
        "        if (batch_idx + 1) % save_every_n_batches == 0:\n",
        "            state = {\n",
        "                \"last_batch\": batch_idx,\n",
        "                \"accumulated_loss\": accumulated_loss,\n",
        "                \"total_tokens\": total_tokens\n",
        "            }\n",
        "            torch.save(state, eval_ckpt_path)\n",
        "            print(f\" Saved eval state at batch {batch_idx}  tokens={total_tokens}\")\n",
        "\n",
        "    final_avg_loss = accumulated_loss / total_tokens\n",
        "    test_perplexity = torch.exp(torch.tensor(final_avg_loss)).item()\n",
        "    os.remove(eval_ckpt_path)\n",
        "\n",
        "    print(f\"\\n Test complete. Avg. token-loss = {final_avg_loss:.4f}\")\n",
        "    print(f\" Test Perplexity = {test_perplexity:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQDsV-87xGwc"
      },
      "source": [
        " Test complete. Avg. token-loss = 2.7282\n",
        " Test Perplexity = 15.30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrJq7Z5ipqlq"
      },
      "outputs": [],
      "source": [
        "# 2. LATENCY\n",
        "\n",
        "import time\n",
        "\n",
        "n_samples = min(50, len(test_prompts))\n",
        "max_new_tokens = 128\n",
        "batch_size = 4\n",
        "\n",
        "\n",
        "tokenizer.padding_side = \"left\"\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "latencies = []\n",
        "model.eval()\n",
        "device = model.device\n",
        "\n",
        "\n",
        "dummy_input = tokenizer(\"Hello\", return_tensors=\"pt\").to(device)\n",
        "_ = model.generate(\n",
        "    **dummy_input,\n",
        "    max_new_tokens=10,\n",
        "    do_sample=False,\n",
        "    use_cache=True,\n",
        "    return_dict_in_generate=False\n",
        ")\n",
        "torch.cuda.synchronize()\n",
        "\n",
        "\n",
        "for i in range(0, n_samples, batch_size):\n",
        "    batch_prompts = test_prompts[i : i + batch_size]\n",
        "    inputs = tokenizer(\n",
        "        batch_prompts,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=True\n",
        "    ).to(device)\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    start = time.time()\n",
        "    _ = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=False,\n",
        "        use_cache=True,\n",
        "        return_dict_in_generate=False\n",
        "    )\n",
        "    torch.cuda.synchronize()\n",
        "    end = time.time()\n",
        "\n",
        "\n",
        "    elapsed = end - start\n",
        "    latencies.append(elapsed / len(batch_prompts))\n",
        "\n",
        "avg_latency = sum(latencies) / len(latencies)\n",
        "print(f\"Average Latency (per prompt, {max_new_tokens} new tokens): {avg_latency:.4f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nD9U6-j2xNrp"
      },
      "source": [
        "Average Latency (per prompt, 128 new tokens): 7.2467 seconds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gR4dCQIp2rX"
      },
      "outputs": [],
      "source": [
        "# 3. MODEL SIZE ON DISK\n",
        "\n",
        "def folder_size_in_mb(path: str) -> float:\n",
        "    total_bytes = 0\n",
        "    for root, _, files in os.walk(path):\n",
        "        for fname in files:\n",
        "            fp = os.path.join(root, fname)\n",
        "            total_bytes += os.path.getsize(fp)\n",
        "    return total_bytes / (1024 ** 2)\n",
        "\n",
        "model_size_mb = folder_size_in_mb(latest_checkpoint)\n",
        "print(f\"Model Size on Disk (under {latest_checkpoint}): {model_size_mb:.1f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5YZGoQOxQST"
      },
      "source": [
        "Model Size on Disk (under /content/drive/MyDrive/medical_qlora/checkpoint-375): 161.2 MB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULJOixBzqv66"
      },
      "outputs": [],
      "source": [
        "# Generation for ROUGE calculation\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def optimized_medical_generation(\n",
        "    model, tokenizer, prompts, references,\n",
        "    checkpoint_path, batch_size=8, max_length=512\n",
        "):\n",
        "\n",
        "\n",
        "    if os.path.isdir(checkpoint_path):\n",
        "        os.makedirs(checkpoint_path, exist_ok=True)\n",
        "        state_file = os.path.join(checkpoint_path, \"generation_resume.json\")\n",
        "    else:\n",
        "        state_file = checkpoint_path\n",
        "\n",
        "\n",
        "    if os.path.exists(state_file):\n",
        "        with open(state_file, 'r') as f:\n",
        "            state = json.load(f)\n",
        "        completed_indices = state.get('completed_indices', [])\n",
        "        predictions = state.get('predictions', [])\n",
        "        print(f\"Resuming from {len(completed_indices)} completed samples\")\n",
        "    else:\n",
        "        completed_indices = []\n",
        "        predictions = []\n",
        "        state = {'completed_indices': completed_indices, 'predictions': predictions}\n",
        "\n",
        "    completed_set = set(completed_indices)\n",
        "    remaining_indices = [i for i in range(len(prompts)) if i not in completed_set]\n",
        "\n",
        "    if not remaining_indices:\n",
        "        print(\"All samples already processed!\")\n",
        "        return predictions, [references[i] for i in completed_indices]\n",
        "\n",
        "    model.eval()\n",
        "    device = model.device\n",
        "\n",
        "\n",
        "    tokenizer.padding_side = \"left\"\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        emb = model.get_input_embeddings().weight\n",
        "        emb[tokenizer.pad_token_id].zero_()\n",
        "\n",
        "\n",
        "    for batch_start in tqdm(range(0, len(remaining_indices), batch_size),\n",
        "                            desc=\"Medical Generation\"):\n",
        "        batch_end     = min(batch_start + batch_size, len(remaining_indices))\n",
        "        batch_indices = remaining_indices[batch_start:batch_end]\n",
        "        batch_prompts = [prompts[i] for i in batch_indices]\n",
        "\n",
        "\n",
        "        inputs = tokenizer(\n",
        "            [f\"MEDICAL PROMPT: {p}\" for p in batch_prompts],\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=max_length,\n",
        "            return_attention_mask=True,\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                input_ids      = inputs[\"input_ids\"],\n",
        "                attention_mask = inputs[\"attention_mask\"],\n",
        "                pad_token_id   = tokenizer.pad_token_id,\n",
        "                max_new_tokens = 256,\n",
        "                do_sample      = False,\n",
        "                num_beams      = 1,\n",
        "                use_cache      = True,\n",
        "            )\n",
        "\n",
        "        decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "        batch_preds = []\n",
        "        for prompt, text in zip(batch_prompts, decoded):\n",
        "            prefix = f\"MEDICAL PROMPT: {prompt}\"\n",
        "            if text.startswith(prefix):\n",
        "                gen_text = text[len(prefix):].strip()\n",
        "            else:\n",
        "                gen_text = text.strip()\n",
        "            batch_preds.append(gen_text)\n",
        "\n",
        "        predictions.extend(batch_preds)\n",
        "        completed_indices.extend(batch_indices)\n",
        "\n",
        "\n",
        "        with open(state_file, 'w') as f:\n",
        "            json.dump({\n",
        "                'completed_indices': completed_indices,\n",
        "                'predictions': predictions\n",
        "            }, f)\n",
        "\n",
        "    return predictions, [references[i] for i in completed_indices]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ugUsyOKrElV",
        "outputId": "00a85b4d-2335-4fb6-fee0-e38b623de0fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resuming from 1000 completed samples\n",
            "All samples already processed!\n"
          ]
        }
      ],
      "source": [
        "predictions, processed_refs = optimized_medical_generation(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    test_prompts,\n",
        "    test_references,\n",
        "    latest_checkpoint,\n",
        "    batch_size=12,\n",
        "    max_length=120\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400,
          "referenced_widgets": [
            "9687c1986ef8424eb8f8afaefe85a63c",
            "f5fcde7e098a42edb4f567c07227442f",
            "b60364023bcf4993b29c74a77b535725",
            "457959c6925244d5a2438d237a5cf2c1",
            "9ee0c047530a48b78611b517348b1cea",
            "df38682969f54a0596cf80941f4045e6",
            "0bdd47e4b22649c0a9b11476f2389129",
            "d58640e9287343e2b4d1f4af247e6aca",
            "cac130f60bd04db58c39cc15e683f505",
            "b1c5063e85774a149a7dd37da386eda6",
            "fe6ea91ae7454a7aa2e2acad33619c07"
          ]
        },
        "id": "SGpnlwWqrJHL",
        "outputId": "2a5329bc-0925-463e-b5e5-917adbd4ec72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=8680567035a0fe0aebd7875e4bd6a4ae636c080056b2461c4032b88e3139778b\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9687c1986ef8424eb8f8afaefe85a63c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 4. ROUGE score\n",
        "!pip install rouge_score\n",
        "from evaluate import load\n",
        "rouge = load(\"rouge\")\n",
        "results = rouge.compute(\n",
        "        predictions=predictions,\n",
        "        references=processed_refs,\n",
        "        use_stemmer=True,\n",
        "        use_aggregator=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qty_BrK_rReE",
        "outputId": "4b2ed2a1-c049-424e-ac90-13e522d6666f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Medical ROUGE Scores:\n",
            "ROUGE-1: 0.2398\n",
            "ROUGE-2: 0.0307\n",
            "ROUGE-L: 0.1212\n",
            "ROUGE-Lsum: 0.1469\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nMedical ROUGE Scores:\")\n",
        "print(f\"ROUGE-1: {results['rouge1']:.4f}\")\n",
        "print(f\"ROUGE-2: {results['rouge2']:.4f}\")\n",
        "print(f\"ROUGE-L: {results['rougeL']:.4f}\")\n",
        "print(f\"ROUGE-Lsum: {results['rougeLsum']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsI-kcEbrToE"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/drive/MyDrive/medical_rouge_results.json\", \"w\") as f:\n",
        "        json.dump({\n",
        "            \"scores\": results,\n",
        "            \"predictions\": predictions,\n",
        "            \"references\": processed_refs\n",
        "        }, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4yvo8ddxkV6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}