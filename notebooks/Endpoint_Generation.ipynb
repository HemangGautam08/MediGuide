{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vdyZimbsg8Dq"
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate bitsandbytes\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KjJ9x7xuYqXm"
   },
   "source": [
    "# Initializing Gradio interface for Prompt-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290,
     "referenced_widgets": [
      "3ba1ea11ec5d4d9c8b3eb2d7e0f4e95c",
      "1a1fb0e277074cb28c68887a03c60a5f",
      "19b17f6079404e2aa4484a559556fa3e",
      "4ca8fe0a6bcf4c45b06ad40335206afa",
      "e622dd318fcc4384bef845ab1d0566f7",
      "3d7bb138224a4a2d94dc264717fc5a1f",
      "8b4edca6d46f4441be08f7f4559cfbc6",
      "973473c0d3d24e3086dec6c0339ad68b",
      "1b64c4620e1c4ebbb342471a587e5128",
      "ad3e01de88534e70ae56a59853f7ce71",
      "3bdf876a33ca4b51ac1acdce00cc5128",
      "a8ea5f7ec99d4b8a9de6583d2c091bbb",
      "b406b69f1b4f416593d618c90cb579e6",
      "35f0a7a167b142b2be4141fb5116d3f9",
      "ac6ff78ed4404403b4bfcd7c640045b5",
      "e0b98d230f584de0bc3ffe002ff8b577",
      "925486adc3ac46e3912a55817a92c1b8",
      "be71e28906144184a3dbf566aebe11de",
      "9fc3c3f7d139406ba1f334d9b91294b3",
      "33d25da0e23b47138eb0495a5caa3bd7",
      "7a73ecd9218e43e9b9889ac40be45fe4",
      "0e4b2b7e1e8d4b188590af67e953a3ce",
      "3b7502194c8b466fbbadf26ef08c7efc",
      "b9017060bf4447db8f3f195ec04f0a14",
      "78cdce2897824859be6f5d9f404288f1",
      "f0c0e510cc014d3fb8ea151e15020c64",
      "a075d75993954db582b1b31392e97407",
      "2d271e07ac104ce0bb04955589332070",
      "11fa210536a14f80a039fa2e59efff0a",
      "382d8fa6bece4c679f3eccc3e2ab412b",
      "1208bb6f61314d77955bf8ff479814cf",
      "cb6372ac00d542d39dacc187b5b707fe",
      "b3e7269960bd46c199d5ab910955bc88",
      "13ba97f13df748dbb3911a54a764ff1a",
      "ea2f915102094606a00846e5b4393cf5",
      "77feee59269942a6b55a786b7d288c8e",
      "8fc500d26889476d9e68478f43029917",
      "1679b7e1c4304c7f92dcf48ece86eded",
      "3491e90b207448a4864dd262772e0464",
      "b63ec37b6d3243d7a7ec50660f02862d",
      "0f915355e1044f87b18a9221d34cec33",
      "a1dda327b16b49aab27cc9f2756deefc",
      "1f985fccdf1a4565b07e5d0592b3e09c",
      "a244983930b64263ac9df6aba37ad79b",
      "7f2473815ee44cc29611dec4268c0ba1",
      "4e64bf06279f48efa6d37b030e0a8484",
      "fd76b096edad44788d32acf807ca4c6a",
      "4630f2cd38dd49baab1551b791f70280",
      "115a617d09854ce68ca2460bb813a08f",
      "0d9f212bce28459cba2906aacf9f97c7",
      "c784d14bf8694b70ba5c9adad05f3a2b",
      "0d74160fe4974dac93a45dba83842214",
      "1fcc19a6b9a9417aa0856a7d687f9c31",
      "377057e1779d4306852c4b478bfb3e22",
      "f16191c796864a0cadfe28b5724ff4eb",
      "cd10ea52988249ebaf6f632923cee857",
      "2dcc618e2d514dd29562a6a9b7389df3",
      "3cd2dad35397407a99902b962593df9a",
      "3f9bf37ccc6b466a9ed7023fd1c93401",
      "1ef687e5f89a49b282daee6ffda0d1c8",
      "ff96c16920564176bd550a262250cf9e",
      "2edd4122c5984e228b50acac7f86c5fd",
      "e4a3d2108be542d291643df2523a3fca",
      "476cab6fe96e48fb98e0b4281158c681",
      "0c262846d89b4bc182abdd4b3ea351ee",
      "88099b2227904c53aefc9111a6902be4",
      "122d3fdd30f747f197d6531ceba63120",
      "c1b4330f7ae7490ab9a301c7fec20f63",
      "d6e4370cc01648ba829942dcaa21154f",
      "bf3c09dd1e9742d489e6e507ad960dd4",
      "db901fbe9e0e42aeb7e8958af2dd3bae",
      "feef49f94a5548948395b7f0620b002f",
      "d9cd43cedda64239b1161c7f37f7c77e",
      "68e98225ccdb41288a00eacce630712a",
      "4afef29f709b4ae88646c7df762af140",
      "8cef28f399f7403a93e69bd96fb46476",
      "e30a9978a6ad46c18da6c97ea023a832",
      "3bf8a99916e64ea89e58bfceb996db14",
      "ed5aabb174864015a36d6132cf956aea",
      "145d29a4d5e645fea26da45c1a12bf11",
      "9b9b7c45c6d1411297b0bd25575add77",
      "c1dad4a8007a46d8a64851bae1059cb9",
      "849b2e0cc1b442b9b1b1ed22b74020c4",
      "9e19f479711e4e23bfb0e4b2d0639804",
      "9a9f46d5352f4ddf832e2460be7a492d",
      "9b7f2176d3bb42659cb80575e8c59891",
      "1ba1b5cbda1a4d658c4c4df9a0d55fa9",
      "461f79a9093148a39b4639abee62d203"
     ]
    },
    "id": "byUQum-ZhFk4",
    "outputId": "dab88ca7-4d5b-45a2-b123-e83104144f33"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ba1ea11ec5d4d9c8b3eb2d7e0f4e95c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/137k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8ea5f7ec99d4b8a9de6583d2c091bbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b7502194c8b466fbbadf26ef08c7efc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/3.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13ba97f13df748dbb3911a54a764ff1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/437 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f2473815ee44cc29611dec4268c0ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja:   0%|          | 0.00/3.96k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd10ea52988249ebaf6f632923cee857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.09k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "122d3fdd30f747f197d6531ceba63120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.14G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bf8a99916e64ea89e58bfceb996db14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gabbar427/mediguide\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gabbar427/mediguide\")\n",
    "\n",
    "\n",
    "chatbot = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "def chat_with_bot(message, max_new_tokens=2000):\n",
    "    # Format the input with special tokens if required\n",
    "    inputs = f\"<|user|>\\n{message}\\n<|assistant|>\\n\"\n",
    "\n",
    "    # Generate response\n",
    "    outputs = chatbot(\n",
    "        inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.1,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Extract and clean the response\n",
    "    response = outputs[0]['generated_text'].split(\"<|assistant|>\")[1].strip()\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 611
    },
    "id": "Y4JvsWEElwqn",
    "outputId": "47ea2b1e-e46d-477a-cbfa-9d23118707f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "* Running on public URL: https://d9b6ec1eb15e39f318.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://d9b6ec1eb15e39f318.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=chat_with_bot,\n",
    "    inputs=gr.Textbox(lines=3, label=\"Your Medical Question\"),\n",
    "    outputs=gr.Textbox(label=\"MediGuide Bot\"),\n",
    "    title=\"🩺 MediGuide Medical Chatbot\",\n",
    "    description=\"Ask health-related questions. Powered by gabbar427/mediguide.\"\n",
    ")\n",
    "\n",
    "# Launch it!\n",
    "iface.launch(share=True)  # share=True gives you a public URL\n",
    "\n",
    "# Add the following url as endpoint for mistralai_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUz9xk9Wal8b"
   },
   "source": [
    "# Initializing Gradio interface for Prefix Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "lrf-5IOYbhXH"
   },
   "outputs": [],
   "source": [
    "path = \"ankraj/mediguide\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465,
     "referenced_widgets": [
      "7262350ce5be491c949cc9da5510be24",
      "a3d92c58854048c18e0575885e38fa7b",
      "cd2c49e083194dd9ade65cd87b7b37df",
      "eb556b21cc484f228b1dfa6ae71a66d8",
      "11ac293f96824b76aa65aaccc9766588",
      "e99459cbb2ce48238c62768b44dee6be",
      "35f42a57ad16464299ab5cb0a06eccae",
      "939827b84bf2471ba919f04760c2a99d",
      "7046db85752f4ef79067d648bf2a8507",
      "cb9c4ce87dfd4bf8af0348c10b59a5f5",
      "9915039d37824d10956047c036eee1d6",
      "83b0319282154f0cb7b582c5cf7fc3b2",
      "93ac8afce6b94228804818c4f393f3b5",
      "5799f90ea634479a95a43f9a270993f6",
      "3f7d994225bc44bf9dae46d0652475b4",
      "8735d20af44249c48a36c2d5540757f4",
      "9e5ba9f297c84c28912f0f246108baf4",
      "8e1b01a8fd73438e9784d49f914d8672",
      "c0b49270f63b4115a198db9344a8ccef",
      "2c07e6b033c9438ca3c61aa4c12a3ba2",
      "37faf73fb0cf4744a793dc97d8e4a5b1",
      "6ee44efec4eb4009958fdbdf4e1e1e1b",
      "359746a447c74902bc3274e0f46b6c0f",
      "faf1f18261b94852815e2791f74cb7e1",
      "05d61f7175884fd8b4d300f598a6d18a",
      "a27c027dbace4ecaaef325ef1f06bef5",
      "cc195cf7257843d581c302caf5eb746d",
      "ef1ae016670a408faad809705db05c1b",
      "aaf776f27b824bee8c45d5586e66f26e",
      "512ecdcbcd354ee4a2df2f8de90631dc",
      "83cec5349ef84ef28862bb6a5ddc6576",
      "7ef60c0108684def80457bc0fbc508c6",
      "5491075c195b492783fe63acbb13c052",
      "417d0d1799eb42508c72710bdfe906e5",
      "acef6f435395424d9d450e909104c5a6",
      "ec25b1d321b94068aaf53a136c4116d2",
      "9e2319f325b541fab2182d8dca56b998",
      "474da854775d40728be1855b4accecce",
      "4427207f4ad14befbcdc1f0cbad83815",
      "bccd462a353a4222a1b6bdeb9919e95f",
      "16fad727eefc4ce39131d1d24c5dc1b7",
      "7de7dbaff5b94bd2898d5127d848b9c9",
      "7eb0e5dd9aad475f9524d7799ac27cfe",
      "b4a0f3972c5345bbb246e5ec352809eb",
      "c7a961847ab94296bb02abd245e950ca",
      "929b6a8e3fab485d821179952ca91f10",
      "6c1b201445ce497aaa1b1caf27d62e58",
      "554f1e86b22146049e508c537845a316",
      "aa2b35dd295c44278020fa8a4f4ddccf",
      "c7747d2245cf431e9dcd95e163f537f7",
      "fd23321afaf1440a96f5cbb94948f8ea",
      "356a0f0ca85a4189b16b4f96a94323a7",
      "fc0371082a2448f181332a7dc5081c94",
      "47702cb41e2b4082bfa7b36b312e001f",
      "1f714d86653b49c29555b6066e2b921e",
      "94909b715af84d4fbd5831e1c9a175ee",
      "fd6aa6f14bc445a88bff0588b8ea8315",
      "780f64337f4648d7bd68b92d71c0ab58",
      "89cfe685cf94413eaab1c531a592e483",
      "7168281ce79043c9b868c4f5939a47e7",
      "168faa2a95a24bfbaa7da4e15991547d",
      "142586e19d5d4f88afa74d6d5bd193d3",
      "14899c7835ed41b2b398f45b07d4f65e",
      "3c3e0651588a4800b9a24a19330792eb",
      "6fb2404d38ba43ac9426f20f77a2cc12",
      "fc8a0adcdb07460f843566f1320f6997",
      "da80bbf402f74fbc8d216bf0735481c9",
      "c6f126acb3424ba3af0fd8661d55653b",
      "501281d177b84d5b8e93d19e38bbe207",
      "e6f03cf59d5046c28e3e7db5719ff181",
      "4331fe0be8a0403e99962909492aaa06",
      "2b8b3f4cd81d4bf9b64e3f60df0a5e22",
      "ffad1a33bb4c4f6bbbb0df1503303c3a",
      "f89a758432b34db28f45641814811e81",
      "a8d318c770e14f0cbd163d54ce7602b8",
      "cfb1cba642cc43a4b3d1db6b838fd392",
      "92a2e62a673d479eba9d3c5933b0034c",
      "ab1a87d82df043f0a9ffb8469d7ea240",
      "27bcce45c861493190ea07c845e4c5b8",
      "cdc5bcd1fd0844909e3f29ac14b9a8ec",
      "ae426b3d2d344c33824d5e3ec3bd0620",
      "ca79010b328448569f80eeac75904e70",
      "fed98180d35b4ac299626b4727722457",
      "322541e446614cf8a5ac9ab6b8625995",
      "302b1c0f5fea44f79d11392b15cd6583",
      "82d4cd8aa68e443d953d8ea9dbdfad89",
      "66e00b7b25af44ffb2ed108d42477156",
      "f78870231c414a77bac6b57ec27242b1",
      "69010ae39c044356b0298b97a12b90cb",
      "21aec7e0208b4d4ab0afb3396e08bb26",
      "f17594f5ce4c4bbab25fb28e25a8b8c5",
      "67da50b769904e488349bc88d452e4a1",
      "d12d6685e6dc48deac5f253abbf79016",
      "888d4ccc29594e75a7a70c1f531277f0",
      "d932f73d743648c38583c5bf2e62286a",
      "2c2d94478e33461fa7ec4d7438e7e161",
      "8d6ff6ddfbc14a21a043e1cb1dee3b55",
      "9145f9617bf046d7a228417ecd3fe3a4",
      "996918fe381f4f85826f7c08e524b46b",
      "2c9d784b7f504a64a5c1fc9f44d20034",
      "8a1d38f523db42e4bdc7817e278d0182",
      "763ac64a3b1e484a91ac514815ae4d62",
      "d7c5283eff5b464daf96e379bdb25b23",
      "91677efe7e2e456a95530079c726d0e2",
      "694019de75ad4cb8a17cac6557cd9c73",
      "337bfcad35284adea0d78158bbd1917e",
      "129ee5210bf34f6ca193614e48bc2022",
      "77617f7af571401cb6c5a81baba71cdd",
      "81494b49d6e44b8dbcbc32d6c7ad5497",
      "a9ecd2e1ca9e4015a7046c7ef39cef16",
      "c79f2bc625144fb9bb994d7e583f4a17",
      "816c68db54f14f27a102bfa656288141",
      "39944cdbaee846fb9b52eb64afe815a4",
      "0e689369fe10454a92c1daeeeff0a1ca",
      "e1c6c5045861433383306bd3f30ef0a7",
      "8e45f463aac34f588e9730f37ca4426e",
      "5039de1b40c3438ea4a0c75ebf238956",
      "4824edc14d764d379c5707e648cf63e0",
      "841cb8f05e084448ab9c0a4b0adbfafa",
      "efa2014e2ba14f849ace27e71dc24423",
      "76d3ef2f677d412f842c1d47fafdb76e",
      "abd3a677a568464f951244b94748f17f",
      "87e9c7e651cd483aafe40ad304559a91",
      "7371a1687038443a9c6f989d0826e8f1",
      "4567ded0bc4740be9338fb9baf50365d",
      "d53332b1c8714913aefa79d72425f37c",
      "705888838ac247c38a777a8f0f4bfbe4",
      "f5c1cddca9f54d119b50d8c174f3c072",
      "3174982f23a84211b3da47874fdcc89b",
      "1cf3660f02074b7d944f332d71dfb2e9",
      "f975b1e800654edca6ba2f03708a7895",
      "92964835b9d5464f98088b52f1580887",
      "29d3de3d423a4bee8f1139cc9bc21cb5",
      "6026259efc804d8cba4e1a1a3066c6d6",
      "01a02540cb3a4140af7dac96a5dc63c1",
      "d09dea33e58643278f91742104be258f",
      "fcda16518bc54e60b249903067743e41",
      "5cc39647cc4a4efeb22e2edefbdbf37d",
      "75becba092fc4f399ab18cba16482f4a",
      "f4f41fb84b0f4685a1d9f4016eba4c88",
      "6c04e0a9c2e4497fb218cb8a262a1023",
      "873e5c81637a4d85bed9eabb87671af1",
      "e14f94a2a3c74dc7b90c8466025f55fa",
      "76529b914baa4bcdacf6c9fbe2b01ed5",
      "75b8dfdb28294a50a9ccc8cc5291ca56",
      "30175c270b5240a89af53c179dfa6f62",
      "b8b40e2b04a048dfae534bc9389988bc",
      "81ca34f0a2ce45b29c1e6cf700a185c5",
      "fab30aa4f435443b843af2db015b51e6",
      "a6e388f70433440fbd4c18991cad1021",
      "93c0441a32954a74a5138a3819f2e2e6",
      "0b035f2d75c94faa8b877b8a72a5c1bf",
      "e6eb5bdbd93a401dbe6fe046c3a6d506",
      "51a2d45103bd4a609638f3b2454847de"
     ]
    },
    "id": "RWRB8BPna7CT",
    "outputId": "c1e06f1f-7580-4355-8542-d43dbf607f6f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7262350ce5be491c949cc9da5510be24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/601 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83b0319282154f0cb7b582c5cf7fc3b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "359746a447c74902bc3274e0f46b6c0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "417d0d1799eb42508c72710bdfe906e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7a961847ab94296bb02abd245e950ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.55G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94909b715af84d4fbd5831e1c9a175ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da80bbf402f74fbc8d216bf0735481c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab1a87d82df043f0a9ffb8469d7ea240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69010ae39c044356b0298b97a12b90cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/391 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9d784b7f504a64a5c1fc9f44d20034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/5.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c79f2bc625144fb9bb994d7e583f4a17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/141k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abd3a677a568464f951244b94748f17f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29d3de3d423a4bee8f1139cc9bc21cb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/3.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76529b914baa4bcdacf6c9fbe2b01ed5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/437 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Load base model first\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    offload_folder=\"./offload\",\n",
    "    offload_state_dict=True\n",
    ")\n",
    "\n",
    "# Inject adapter\n",
    "model = PeftModel.from_pretrained(base_model, path)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "cqOgnHyXbwi5"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "IL39Diyob1cD"
   },
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __init__(self, stop_phrases, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stop_ids_list = [\n",
    "            tokenizer(phrase, return_tensors=\"pt\").input_ids[0][1:]  # remove BOS\n",
    "            for phrase in stop_phrases\n",
    "        ]\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        device = input_ids.device\n",
    "        for stop_ids in self.stop_ids_list:\n",
    "            stop_ids = stop_ids.to(device)  # ✅ Move to same device\n",
    "            if len(input_ids[0]) >= len(stop_ids):\n",
    "                if torch.equal(input_ids[0][-len(stop_ids):], stop_ids):\n",
    "                    return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "5gVmkkdwb6BG"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocess_input(input_text):\n",
    "    instruction = \"If you are a doctor, please answer the medical questions based on the patient's description.\"\n",
    "    prompt = f\"[MED] {instruction}\\nPatient: {input_text} \\nDoctor:\"\n",
    "    return prompt\n",
    "\n",
    "def clean_output(text):\n",
    "    stop_patterns = [\n",
    "        r\"Take care Chat Doctor\\.\",\n",
    "        r\"Regards, Chat Doctor\\.\",\n",
    "        r\"Regards. Chat Doctor\\.\",\n",
    "        r\"Wishing you good health\\.\",\n",
    "        r\"Wishing you a good health\\.\",\n",
    "        r\"Thanks for using Chat Doctor\\.\",\n",
    "        r\"Goodbye\\.\",\n",
    "        r\"Take care\\.\",\n",
    "        r\"\\.com\"\n",
    "    ]\n",
    "\n",
    "    doc_match = re.search(r\"Doctor:\\s*(.*)\", text, re.DOTALL | re.IGNORECASE)\n",
    "    if not doc_match:\n",
    "        return text.strip()\n",
    "\n",
    "    after_doctor = doc_match.group(1)\n",
    "\n",
    "    stop_pattern = r\"(.*?)(\" + \"|\".join(stop_patterns) + \")\"\n",
    "    stop_match = re.search(stop_pattern, after_doctor, re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "    if stop_match:\n",
    "        return stop_match.group(1).strip() + \" \" + stop_match.group(2)\n",
    "\n",
    "    return after_doctor.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "WgN3qUQbcGV7"
   },
   "outputs": [],
   "source": [
    "def run_medical_bot(message, max_new_tokens=500):\n",
    "    prompt = preprocess_input(message)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # List of phrases that should stop generation\n",
    "    stop_phrases = [\n",
    "        \"Take care Chat Doctor.\",\n",
    "        \"Regards, Chat Doctor.\",\n",
    "        \"Regards. Chat Doctor.\",\n",
    "        \"Wishing you good health.\",\n",
    "        \"Wishing you a good health.\",\n",
    "        \"Thanks for using Chat Doctor.\",\n",
    "        \"Goodbye.\",\n",
    "        \"Take care.\"\n",
    "    ]\n",
    "\n",
    "    stopping_criteria = StoppingCriteriaList([StopOnTokens(stop_phrases, tokenizer)])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            stopping_criteria=stopping_criteria\n",
    "        )\n",
    "\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return clean_output(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 611
    },
    "id": "LVNIqOHecSOk",
    "outputId": "c9411fd6-e934-4ff5-b028-1afcb2273a71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "* Running on public URL: https://13b6b5feadc45ec9eb.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://13b6b5feadc45ec9eb.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "iface2 = gr.Interface(\n",
    "    fn=run_medical_bot,\n",
    "    inputs=gr.Textbox(lines=3, label=\"Your Medical Question\"),\n",
    "    outputs=gr.Textbox(label=\"MediGuide Bot\"),\n",
    "    title=\"🩺 MediGuide Medical Chatbot\",\n",
    "    description=\"Ask health-related questions. Powered by ankraj/mediguide.\"\n",
    ")\n",
    "\n",
    "iface2.launch(share=True)\n",
    "# Add the following url as endpoint for mistralai_prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-AAUMKEZi0W"
   },
   "source": [
    "# Initializing Gradio interface for QLoRA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156,
     "referenced_widgets": [
      "0bab1592e0bd4e8a969ff330dbcb8216",
      "6f696a9d3dce47198e2f8f31d777a902",
      "504ed95274dc4897aa17734ea0a19d4a",
      "1400bde49558412e94c6a3147f71c807",
      "4ce858a040f745189abdd9c63b2a0ff5",
      "ea6a55078ef44188bffe8d0098eeab5c",
      "e0982d17fece4876aa0aae03e5433342",
      "46d411edd3f84569afe892850931423a",
      "f0dda12bae0c4b81ad3c81274f1c2f10",
      "0552f16b8a8b46de9008c0f4fc7ed5f9",
      "05eae139115144a7a14610629c74dcf2"
     ]
    },
    "id": "U-rrcufVZmN-",
    "outputId": "cc43c017-d817-4015-f035-0971befc8c18"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bab1592e0bd4e8a969ff330dbcb8216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Greyitis/mediguide_new were not used when initializing MistralForCausalLM: ['model.layers.0.self_attn.q_proj.base_layer.weight', 'model.layers.0.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.0.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.0.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.0.self_attn.q_proj.lora_A.default.weight', 'model.layers.0.self_attn.q_proj.lora_B.default.weight', 'model.layers.0.self_attn.v_proj.base_layer.weight', 'model.layers.0.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.0.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.0.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.0.self_attn.v_proj.lora_A.default.weight', 'model.layers.0.self_attn.v_proj.lora_B.default.weight', 'model.layers.1.self_attn.q_proj.base_layer.weight', 'model.layers.1.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.1.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.1.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.1.self_attn.q_proj.lora_A.default.weight', 'model.layers.1.self_attn.q_proj.lora_B.default.weight', 'model.layers.1.self_attn.v_proj.base_layer.weight', 'model.layers.1.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.1.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.1.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.1.self_attn.v_proj.lora_A.default.weight', 'model.layers.1.self_attn.v_proj.lora_B.default.weight', 'model.layers.10.self_attn.q_proj.base_layer.weight', 'model.layers.10.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.10.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.10.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.10.self_attn.q_proj.lora_A.default.weight', 'model.layers.10.self_attn.q_proj.lora_B.default.weight', 'model.layers.10.self_attn.v_proj.base_layer.weight', 'model.layers.10.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.10.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.10.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.10.self_attn.v_proj.lora_A.default.weight', 'model.layers.10.self_attn.v_proj.lora_B.default.weight', 'model.layers.11.self_attn.q_proj.base_layer.weight', 'model.layers.11.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.11.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.11.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.11.self_attn.q_proj.lora_A.default.weight', 'model.layers.11.self_attn.q_proj.lora_B.default.weight', 'model.layers.11.self_attn.v_proj.base_layer.weight', 'model.layers.11.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.11.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.11.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.11.self_attn.v_proj.lora_A.default.weight', 'model.layers.11.self_attn.v_proj.lora_B.default.weight', 'model.layers.12.self_attn.q_proj.base_layer.weight', 'model.layers.12.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.12.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.12.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.12.self_attn.q_proj.lora_A.default.weight', 'model.layers.12.self_attn.q_proj.lora_B.default.weight', 'model.layers.12.self_attn.v_proj.base_layer.weight', 'model.layers.12.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.12.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.12.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.12.self_attn.v_proj.lora_A.default.weight', 'model.layers.12.self_attn.v_proj.lora_B.default.weight', 'model.layers.13.self_attn.q_proj.base_layer.weight', 'model.layers.13.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.13.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.13.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.13.self_attn.q_proj.lora_A.default.weight', 'model.layers.13.self_attn.q_proj.lora_B.default.weight', 'model.layers.13.self_attn.v_proj.base_layer.weight', 'model.layers.13.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.13.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.13.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.13.self_attn.v_proj.lora_A.default.weight', 'model.layers.13.self_attn.v_proj.lora_B.default.weight', 'model.layers.14.self_attn.q_proj.base_layer.weight', 'model.layers.14.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.14.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.14.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.14.self_attn.q_proj.lora_A.default.weight', 'model.layers.14.self_attn.q_proj.lora_B.default.weight', 'model.layers.14.self_attn.v_proj.base_layer.weight', 'model.layers.14.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.14.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.14.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.14.self_attn.v_proj.lora_A.default.weight', 'model.layers.14.self_attn.v_proj.lora_B.default.weight', 'model.layers.15.self_attn.q_proj.base_layer.weight', 'model.layers.15.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.15.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.15.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.15.self_attn.q_proj.lora_A.default.weight', 'model.layers.15.self_attn.q_proj.lora_B.default.weight', 'model.layers.15.self_attn.v_proj.base_layer.weight', 'model.layers.15.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.15.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.15.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.15.self_attn.v_proj.lora_A.default.weight', 'model.layers.15.self_attn.v_proj.lora_B.default.weight', 'model.layers.16.self_attn.q_proj.base_layer.weight', 'model.layers.16.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.16.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.16.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.16.self_attn.q_proj.lora_A.default.weight', 'model.layers.16.self_attn.q_proj.lora_B.default.weight', 'model.layers.16.self_attn.v_proj.base_layer.weight', 'model.layers.16.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.16.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.16.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.16.self_attn.v_proj.lora_A.default.weight', 'model.layers.16.self_attn.v_proj.lora_B.default.weight', 'model.layers.17.self_attn.q_proj.base_layer.weight', 'model.layers.17.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.17.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.17.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.17.self_attn.q_proj.lora_A.default.weight', 'model.layers.17.self_attn.q_proj.lora_B.default.weight', 'model.layers.17.self_attn.v_proj.base_layer.weight', 'model.layers.17.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.17.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.17.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.17.self_attn.v_proj.lora_A.default.weight', 'model.layers.17.self_attn.v_proj.lora_B.default.weight', 'model.layers.18.self_attn.q_proj.base_layer.weight', 'model.layers.18.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.18.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.18.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.18.self_attn.q_proj.lora_A.default.weight', 'model.layers.18.self_attn.q_proj.lora_B.default.weight', 'model.layers.18.self_attn.v_proj.base_layer.weight', 'model.layers.18.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.18.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.18.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.18.self_attn.v_proj.lora_A.default.weight', 'model.layers.18.self_attn.v_proj.lora_B.default.weight', 'model.layers.19.self_attn.q_proj.base_layer.weight', 'model.layers.19.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.19.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.19.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.19.self_attn.q_proj.lora_A.default.weight', 'model.layers.19.self_attn.q_proj.lora_B.default.weight', 'model.layers.19.self_attn.v_proj.base_layer.weight', 'model.layers.19.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.19.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.19.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.19.self_attn.v_proj.lora_A.default.weight', 'model.layers.19.self_attn.v_proj.lora_B.default.weight', 'model.layers.2.self_attn.q_proj.base_layer.weight', 'model.layers.2.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.2.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.2.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.2.self_attn.q_proj.lora_A.default.weight', 'model.layers.2.self_attn.q_proj.lora_B.default.weight', 'model.layers.2.self_attn.v_proj.base_layer.weight', 'model.layers.2.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.2.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.2.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.2.self_attn.v_proj.lora_A.default.weight', 'model.layers.2.self_attn.v_proj.lora_B.default.weight', 'model.layers.20.self_attn.q_proj.base_layer.weight', 'model.layers.20.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.20.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.20.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.20.self_attn.q_proj.lora_A.default.weight', 'model.layers.20.self_attn.q_proj.lora_B.default.weight', 'model.layers.20.self_attn.v_proj.base_layer.weight', 'model.layers.20.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.20.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.20.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.20.self_attn.v_proj.lora_A.default.weight', 'model.layers.20.self_attn.v_proj.lora_B.default.weight', 'model.layers.21.self_attn.q_proj.base_layer.weight', 'model.layers.21.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.21.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.21.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.21.self_attn.q_proj.lora_A.default.weight', 'model.layers.21.self_attn.q_proj.lora_B.default.weight', 'model.layers.21.self_attn.v_proj.base_layer.weight', 'model.layers.21.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.21.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.21.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.21.self_attn.v_proj.lora_A.default.weight', 'model.layers.21.self_attn.v_proj.lora_B.default.weight', 'model.layers.22.self_attn.q_proj.base_layer.weight', 'model.layers.22.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.22.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.22.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.22.self_attn.q_proj.lora_A.default.weight', 'model.layers.22.self_attn.q_proj.lora_B.default.weight', 'model.layers.22.self_attn.v_proj.base_layer.weight', 'model.layers.22.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.22.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.22.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.22.self_attn.v_proj.lora_A.default.weight', 'model.layers.22.self_attn.v_proj.lora_B.default.weight', 'model.layers.23.self_attn.q_proj.base_layer.weight', 'model.layers.23.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.23.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.23.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.23.self_attn.q_proj.lora_A.default.weight', 'model.layers.23.self_attn.q_proj.lora_B.default.weight', 'model.layers.23.self_attn.v_proj.base_layer.weight', 'model.layers.23.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.23.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.23.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.23.self_attn.v_proj.lora_A.default.weight', 'model.layers.23.self_attn.v_proj.lora_B.default.weight', 'model.layers.24.self_attn.q_proj.base_layer.weight', 'model.layers.24.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.24.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.24.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.24.self_attn.q_proj.lora_A.default.weight', 'model.layers.24.self_attn.q_proj.lora_B.default.weight', 'model.layers.24.self_attn.v_proj.base_layer.weight', 'model.layers.24.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.24.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.24.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.24.self_attn.v_proj.lora_A.default.weight', 'model.layers.24.self_attn.v_proj.lora_B.default.weight', 'model.layers.25.self_attn.q_proj.base_layer.weight', 'model.layers.25.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.25.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.25.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.25.self_attn.q_proj.lora_A.default.weight', 'model.layers.25.self_attn.q_proj.lora_B.default.weight', 'model.layers.25.self_attn.v_proj.base_layer.weight', 'model.layers.25.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.25.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.25.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.25.self_attn.v_proj.lora_A.default.weight', 'model.layers.25.self_attn.v_proj.lora_B.default.weight', 'model.layers.26.self_attn.q_proj.base_layer.weight', 'model.layers.26.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.26.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.26.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.26.self_attn.q_proj.lora_A.default.weight', 'model.layers.26.self_attn.q_proj.lora_B.default.weight', 'model.layers.26.self_attn.v_proj.base_layer.weight', 'model.layers.26.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.26.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.26.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.26.self_attn.v_proj.lora_A.default.weight', 'model.layers.26.self_attn.v_proj.lora_B.default.weight', 'model.layers.27.self_attn.q_proj.base_layer.weight', 'model.layers.27.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.27.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.27.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.27.self_attn.q_proj.lora_A.default.weight', 'model.layers.27.self_attn.q_proj.lora_B.default.weight', 'model.layers.27.self_attn.v_proj.base_layer.weight', 'model.layers.27.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.27.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.27.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.27.self_attn.v_proj.lora_A.default.weight', 'model.layers.27.self_attn.v_proj.lora_B.default.weight', 'model.layers.28.self_attn.q_proj.base_layer.weight', 'model.layers.28.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.28.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.28.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.28.self_attn.q_proj.lora_A.default.weight', 'model.layers.28.self_attn.q_proj.lora_B.default.weight', 'model.layers.28.self_attn.v_proj.base_layer.weight', 'model.layers.28.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.28.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.28.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.28.self_attn.v_proj.lora_A.default.weight', 'model.layers.28.self_attn.v_proj.lora_B.default.weight', 'model.layers.29.self_attn.q_proj.base_layer.weight', 'model.layers.29.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.29.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.29.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.29.self_attn.q_proj.lora_A.default.weight', 'model.layers.29.self_attn.q_proj.lora_B.default.weight', 'model.layers.29.self_attn.v_proj.base_layer.weight', 'model.layers.29.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.29.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.29.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.29.self_attn.v_proj.lora_A.default.weight', 'model.layers.29.self_attn.v_proj.lora_B.default.weight', 'model.layers.3.self_attn.q_proj.base_layer.weight', 'model.layers.3.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.3.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.3.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.3.self_attn.q_proj.lora_A.default.weight', 'model.layers.3.self_attn.q_proj.lora_B.default.weight', 'model.layers.3.self_attn.v_proj.base_layer.weight', 'model.layers.3.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.3.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.3.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.3.self_attn.v_proj.lora_A.default.weight', 'model.layers.3.self_attn.v_proj.lora_B.default.weight', 'model.layers.30.self_attn.q_proj.base_layer.weight', 'model.layers.30.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.30.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.30.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.30.self_attn.q_proj.lora_A.default.weight', 'model.layers.30.self_attn.q_proj.lora_B.default.weight', 'model.layers.30.self_attn.v_proj.base_layer.weight', 'model.layers.30.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.30.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.30.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.30.self_attn.v_proj.lora_A.default.weight', 'model.layers.30.self_attn.v_proj.lora_B.default.weight', 'model.layers.31.self_attn.q_proj.base_layer.weight', 'model.layers.31.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.31.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.31.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.31.self_attn.q_proj.lora_A.default.weight', 'model.layers.31.self_attn.q_proj.lora_B.default.weight', 'model.layers.31.self_attn.v_proj.base_layer.weight', 'model.layers.31.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.31.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.31.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.31.self_attn.v_proj.lora_A.default.weight', 'model.layers.31.self_attn.v_proj.lora_B.default.weight', 'model.layers.4.self_attn.q_proj.base_layer.weight', 'model.layers.4.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.4.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.4.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.4.self_attn.q_proj.lora_A.default.weight', 'model.layers.4.self_attn.q_proj.lora_B.default.weight', 'model.layers.4.self_attn.v_proj.base_layer.weight', 'model.layers.4.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.4.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.4.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.4.self_attn.v_proj.lora_A.default.weight', 'model.layers.4.self_attn.v_proj.lora_B.default.weight', 'model.layers.5.self_attn.q_proj.base_layer.weight', 'model.layers.5.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.5.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.5.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.5.self_attn.q_proj.lora_A.default.weight', 'model.layers.5.self_attn.q_proj.lora_B.default.weight', 'model.layers.5.self_attn.v_proj.base_layer.weight', 'model.layers.5.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.5.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.5.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.5.self_attn.v_proj.lora_A.default.weight', 'model.layers.5.self_attn.v_proj.lora_B.default.weight', 'model.layers.6.self_attn.q_proj.base_layer.weight', 'model.layers.6.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.6.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.6.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.6.self_attn.q_proj.lora_A.default.weight', 'model.layers.6.self_attn.q_proj.lora_B.default.weight', 'model.layers.6.self_attn.v_proj.base_layer.weight', 'model.layers.6.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.6.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.6.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.6.self_attn.v_proj.lora_A.default.weight', 'model.layers.6.self_attn.v_proj.lora_B.default.weight', 'model.layers.7.self_attn.q_proj.base_layer.weight', 'model.layers.7.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.7.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.7.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.7.self_attn.q_proj.lora_A.default.weight', 'model.layers.7.self_attn.q_proj.lora_B.default.weight', 'model.layers.7.self_attn.v_proj.base_layer.weight', 'model.layers.7.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.7.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.7.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.7.self_attn.v_proj.lora_A.default.weight', 'model.layers.7.self_attn.v_proj.lora_B.default.weight', 'model.layers.8.self_attn.q_proj.base_layer.weight', 'model.layers.8.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.8.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.8.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.8.self_attn.q_proj.lora_A.default.weight', 'model.layers.8.self_attn.q_proj.lora_B.default.weight', 'model.layers.8.self_attn.v_proj.base_layer.weight', 'model.layers.8.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.8.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.8.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.8.self_attn.v_proj.lora_A.default.weight', 'model.layers.8.self_attn.v_proj.lora_B.default.weight', 'model.layers.9.self_attn.q_proj.base_layer.weight', 'model.layers.9.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.9.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.9.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.9.self_attn.q_proj.lora_A.default.weight', 'model.layers.9.self_attn.q_proj.lora_B.default.weight', 'model.layers.9.self_attn.v_proj.base_layer.weight', 'model.layers.9.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.9.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.9.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.9.self_attn.v_proj.lora_A.default.weight', 'model.layers.9.self_attn.v_proj.lora_B.default.weight']\n",
      "- This IS expected if you are initializing MistralForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MistralForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MistralForCausalLM were not initialized from the model checkpoint at Greyitis/mediguide_new and are newly initialized: ['model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer1 = AutoTokenizer.from_pretrained(\"Greyitis/mediguide_new\")\n",
    "model1 = AutoModelForCausalLM.from_pretrained(\"Greyitis/mediguide_new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jk9C9cEdaQlL",
    "outputId": "db85635e-7160-4951-fd02-38318945faa9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "chatbot1 = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model1,\n",
    "    tokenizer=tokenizer1,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "def format_prompt(question: str) -> str:\n",
    "\n",
    "    return f\"[|Human|]\\n{question}\\n[|AI|]\\n\"\n",
    "\n",
    "def test_questions(question, max_length: int = 256, **gen_kwargs):\n",
    "    prompt = format_prompt(question)\n",
    "    output = chatbot1(\n",
    "        prompt,\n",
    "        max_length=max_length,\n",
    "        **gen_kwargs\n",
    "    )\n",
    "    full_text = output[0][\"generated_text\"]\n",
    "    response = full_text.split(\"[|AI|]\")[-1].strip()\n",
    "    return response\n",
    "\n",
    "def chat_with_bot1(message):\n",
    "    return test_questions(message, max_length=200, do_sample=True, top_p=0.9, temperature=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 611
    },
    "id": "JnacNZpyajRK",
    "outputId": "acefe440-a020-4e1a-dc3d-1d5d7a3bed71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "* Running on public URL: https://58d9d13f78440deab1.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://58d9d13f78440deab1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "iface1 = gr.Interface(\n",
    "    fn=chat_with_bot1,\n",
    "    inputs=gr.Textbox(lines=3, label=\"Your Medical Question\"),\n",
    "    outputs=gr.Textbox(label=\"MediGuide Bot\"),\n",
    "    title=\"🩺 MediGuide Medical Chatbot\",\n",
    "    description=\"Ask health-related questions. Powered by gabbar427/mediguide.\"\n",
    ")\n",
    "\n",
    "iface1.launch(share=True)\n",
    "# Add the following url as endpoint for mistralai_qlora"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
