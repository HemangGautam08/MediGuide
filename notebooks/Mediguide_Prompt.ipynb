{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5OOZ-Ykb9AWL",
    "outputId": "318e5a2f-c878-45e8-d850-a63e5b347520"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/6.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/6.0 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m114.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m98.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for flash_attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q -U transformers accelerate peft datasets bitsandbytes flash_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E-QqDfk1BF79"
   },
   "outputs": [],
   "source": [
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i4D4dWERCID9"
   },
   "outputs": [],
   "source": [
    "from peft import (\n",
    "    get_peft_model,\n",
    "    PromptTuningConfig,\n",
    "    PromptTuningInit,\n",
    "    TaskType\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NcAsQ9wrCLA7"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CMswTpqMDzA_",
    "outputId": "51763701-3b4d-4505-f2d3-e0afcbc05baf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "drive.mount('/content/drive')\n",
    "file_path = '/content/drive/MyDrive/sampled_6000.json'\n",
    "drive_path = \"/content/drive/MyDrive/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fn4w1hw8D5yu",
    "outputId": "5287ce20-444c-4648-d73e-fd38d0bc9b4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found file at /content/drive/MyDrive/sampled_6000.json\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(file_path):\n",
    "    print(f\"Error: File not found at {file_path}\")\n",
    "    print(\"Available files in directory:\")\n",
    "    print(os.listdir(drive_path))\n",
    "else:\n",
    "    print(f\"Found file at {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6bh_-nVmDvca",
    "outputId": "c7e773ad-f8fa-4a4d-e810-0a32c17e5f7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 6000 medical examples\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "try:\n",
    "    with open(file_path) as f:\n",
    "        try:\n",
    "            medical_data = json.load(f)\n",
    "        except json.JSONDecodeError:\n",
    "\n",
    "            content = f.read().split('[file content end]')[0].split('[file content begin]')[-1].strip()\n",
    "            medical_data = json.loads(content)\n",
    "\n",
    "    print(f\"Successfully loaded {len(medical_data)} medical examples\")\n",
    "\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GWseYc1nF9Ml",
    "outputId": "9ace940d-177a-4866-9b6f-7131c5ebdd55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample data item:\n",
      "Instruction: If you are a doctor, please answer the medical questions based on the patient's description.\n",
      "Input: I wake in the night, usually about 2-3 hours after...\n",
      "Output: Dear patient Here are the possibilities of what yo...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nSample data item:\")\n",
    "print(f\"Instruction: {medical_data[0]['instruction']}\")\n",
    "print(f\"Input: {medical_data[0]['input'][:50]}...\")\n",
    "print(f\"Output: {medical_data[0]['output'][:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z0gWpEoEGI5o"
   },
   "outputs": [],
   "source": [
    "def format_data(sample):\n",
    "        return {\n",
    "            \"text\": f\"<s>[MED] {sample['instruction']}\\nPatient: {sample['input']}\\nDoctor: {sample['output']}</s>\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TwwVWQHwMbnq"
   },
   "outputs": [],
   "source": [
    "dataset = [format_data(d) for d in medical_data]\n",
    "dataset = Dataset.from_list(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DPjb_81Dyfox",
    "outputId": "4a2dda14-f20a-46e9-9452-fdc4166ec0a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "drive.mount('/content/drive')\n",
    "output_dir = \"/content/drive/MyDrive/medical_prompt_tuning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bSelbhVApC1S"
   },
   "outputs": [],
   "source": [
    "def find_latest_checkpoint(output_dir):\n",
    "    try:\n",
    "\n",
    "        if not os.path.exists(output_dir):\n",
    "            print(f\"Output directory {output_dir} does not exist\")\n",
    "            return None\n",
    "\n",
    "\n",
    "        if not os.listdir(output_dir):\n",
    "            print(f\"Output directory {output_dir} is empty\")\n",
    "            return None\n",
    "\n",
    "\n",
    "        checkpoints = [d for d in os.listdir(output_dir)\n",
    "                      if d.startswith(\"checkpoint\") and os.path.isdir(os.path.join(output_dir, d))]\n",
    "\n",
    "        if not checkpoints:\n",
    "            print(\"No checkpoint directories found\")\n",
    "            return None\n",
    "\n",
    "\n",
    "        checkpoints.sort(key=lambda x: int(x.split(\"-\")[1]))\n",
    "        latest = os.path.join(output_dir, checkpoints[-1])\n",
    "        print(f\"Found checkpoint: {latest}\")\n",
    "        return latest\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding checkpoint: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "psgIrj6PrmKi",
    "outputId": "0f7d85c0-fec6-41e0-d735-d5fed84ca9ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found checkpoint: /content/drive/MyDrive/medical_prompt_tuning/checkpoint-1450\n",
      "Latest checkpoint: /content/drive/MyDrive/medical_prompt_tuning/checkpoint-1450\n"
     ]
    }
   ],
   "source": [
    "latest_checkpoint = find_latest_checkpoint(output_dir)\n",
    "print(f\"Latest checkpoint: {latest_checkpoint}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XSRZ4X-4vLkx"
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "config_path= \"/content/drive/MyDrive/medical_prompt_tuning/checkpoint-1450\"\n",
    "config = PeftConfig.from_pretrained(config_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "b21f76b7c9aa4f5cbd4baecad03881ff",
      "3fddd8b6160a41df9437020b390f4ad0",
      "d4ce09a4c29449c98e9dabffb62de500",
      "39d03c47ef7c4221ba3d043524dc0944",
      "9aff7338572248749f982a92ebb5608a",
      "6c65b5a6ab0b4277800e6fcc5d7a28da",
      "5471254f65a045d59a71b1b23795dc9f",
      "3aada29f88c740d896caecf16c70d791",
      "e1fdecfbde0542b282fc026fc04fcde1",
      "e9ff342a4f4147d1b8521019160a4755",
      "b0d1249b02b646c7912685c8d0d44ec0",
      "bed58b41b08041699a625dd4a1c30f13",
      "ed934a8451d54638865136695e0270c1",
      "836db9f65cb447dbac194c03397e977f",
      "c829fbd46576465cb06ddca3eefaf453",
      "4ace18b45cdb4a6481c387128e993476",
      "2b0e68fa22754d7899a7dcd4b0603906",
      "ce7127f2f168440f814f0c47267a9b74",
      "3f7be569395a4bfb9a5d508dbc198ad4",
      "106dc177598a4783a1cb13f046c66239"
     ]
    },
    "id": "-WdLxwItGlVt",
    "outputId": "195d81eb-b327-466c-c248-dc8059837c88"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b21f76b7c9aa4f5cbd4baecad03881ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ReR6kE1xvh-"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ox39IKXUGOaT"
   },
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "810cbd445f3148d695420a1e66f152eb",
      "8b4760ca45a341df8152d018ad8c6512",
      "1f0d01c1e8a6453f84cba5ea635f347e",
      "d72dc31845ca42f5bb05ea25fb94cbc1",
      "fdd0ae2fafc541ac8c81afa3a24d8c78",
      "5b7ff27b8ed044529f110055e368c43d",
      "f25a4a5d760441b3ab6b7289b141b03c",
      "ac19e381dc4f4e7a9f701a21765c0219",
      "624f756fc59b45878604fef1c7b759fd",
      "461263ccb66241f698340932465e10d0",
      "3e8ca5c426fc4b50a8e024d8a0c98414"
     ]
    },
    "id": "aHzR1QsUGR8c",
    "outputId": "6bf6f51b-c035-4d5b-9786-f700871e7097"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "810cbd445f3148d695420a1e66f152eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hsIgrhBoHM0j"
   },
   "outputs": [],
   "source": [
    "mistral = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    latest_checkpoint,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XyyfZQ-BH5ev",
    "outputId": "ed6748de-23ed-4f3a-c691-2acdf42e6c6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 81,920 || all params: 7,248,105,472 || trainable%: 0.0011\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pbSq9iI_ufuk"
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"text\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=256,\n",
    "            return_tensors=None\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "86576abd6e9a4ec489fa7e30ee914b57",
      "f70275b94dae4ee1a3388713f9c53e5b",
      "8ff9d35038364a7482898b60406e9ada",
      "28685bc0d2924539b760e52ff248b7f5",
      "e5135c865f404e33929251235a3baca1",
      "fa7993e3047e4b98b6865064f350e682",
      "6c51b7d2f41c4740bbb0d036793a27e3",
      "a9e63341861d49668effd07be4a686a0",
      "56afd2c518c848899acea75c0bbfc3f9",
      "2423063f3add44c29315c944edb1941b",
      "4388b251e4f74e65a52423e9ad79c82e"
     ]
    },
    "id": "V9f9M5G2O7Py",
    "outputId": "57542d09-42b2-4bb3-da3d-00ec18e23d59"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86576abd6e9a4ec489fa7e30ee914b57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "191c332eac3e48c79bb234149d1ebbab",
      "1ffbad9b2dab4fb694e25474b69752dd",
      "1877645d6956492481da1a0df4c5aa14",
      "6e5902738ee0445f817c8befa604d5d7",
      "5ec0f5eb45744ca0901bfa4237976b7d",
      "d3085f3531f74fb6babf8341a177ca9f",
      "632e40aba75c4f52b6f454cb52391859",
      "04398dedd17048b19cc07a79caa845b3",
      "7574ae142cb24ca69c64446618b454f0",
      "9e11ae00aed3431193d102a3476fb16a",
      "9f907c40710742629b8d807a74954797"
     ]
    },
    "id": "vuCeHQNLH8AK",
    "outputId": "71bb8f3c-58d1-4424-d0a5-f35a1bb8c97d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "191c332eac3e48c79bb234149d1ebbab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "tokenized_dataset = tokenized_dataset.map(\n",
    "        lambda examples: {\"labels\": examples[\"input_ids\"].copy()},\n",
    "        batched=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TYpfEgM3JWuE"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XFuFwqLyIBdC"
   },
   "outputs": [],
   "source": [
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(drive_path, \"medical2_prompt_tuning\"),\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=3e-2,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=20,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    gradient_checkpointing=True,\n",
    "    dataloader_num_workers=2,\n",
    "    remove_unused_columns=True,\n",
    "    max_steps=1900,\n",
    "    label_names=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V-5D72nOrujH"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=mistral,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 873
    },
    "id": "a4vyOK-v2yrE",
    "outputId": "c345fb46-abff-4733-ddfc-3eafb3ab8bee"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1735' max='1900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1735/1900 38:07 < 22:13, 0.12 it/s, Epoch 2.31/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>2.052000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>2.042200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.007600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>2.107600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>2.066900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>2.035100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580</td>\n",
       "      <td>2.013500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.045500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1620</td>\n",
       "      <td>2.082100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>2.057700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1660</td>\n",
       "      <td>2.044400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>2.076000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>2.046900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>2.003800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-2a4e70c57452>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2238\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2239\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2240\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2241\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2242\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2553\u001b[0m                     )\n\u001b[1;32m   2554\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2555\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2557\u001b[0m                     if (\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3789\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scale_wrt_gas\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3791\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3793\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2467\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2468\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2469\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2470\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_lomo_optimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2471\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=latest_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yOm0ZzLS1GmJ"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J2V0HBGcvc3H",
    "outputId": "8157b183-9648-4b1c-d828-ea954dbed8f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/content/drive/MyDrive/medical_prompt_tuning/tokenizer_config.json',\n",
       " '/content/drive/MyDrive/medical_prompt_tuning/special_tokens_map.json',\n",
       " '/content/drive/MyDrive/medical_prompt_tuning/chat_template.jinja',\n",
       " '/content/drive/MyDrive/medical_prompt_tuning/tokenizer.model',\n",
       " '/content/drive/MyDrive/medical_prompt_tuning/added_tokens.json',\n",
       " '/content/drive/MyDrive/medical_prompt_tuning/tokenizer.json')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vy_P62SA_t4g"
   },
   "outputs": [],
   "source": [
    "output_dir = \"/content/drive/MyDrive/medical2_prompt_tuning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzOCHBSeDGcW"
   },
   "outputs": [],
   "source": [
    "def find_latest_checkpoint(output_dir):\n",
    "    try:\n",
    "\n",
    "        if not os.path.exists(output_dir):\n",
    "            print(f\"Output directory {output_dir} does not exist\")\n",
    "            return None\n",
    "\n",
    "\n",
    "        if not os.listdir(output_dir):\n",
    "            print(f\"Output directory {output_dir} is empty\")\n",
    "            return None\n",
    "\n",
    "\n",
    "        checkpoints = [d for d in os.listdir(output_dir)\n",
    "                      if d.startswith(\"checkpoint\") and os.path.isdir(os.path.join(output_dir, d))]\n",
    "\n",
    "        if not checkpoints:\n",
    "            print(\"No checkpoint directories found\")\n",
    "            return None\n",
    "\n",
    "\n",
    "        checkpoints.sort(key=lambda x: int(x.split(\"-\")[1]))\n",
    "        latest = os.path.join(output_dir, checkpoints[-1])\n",
    "        print(f\"Found checkpoint: {latest}\")\n",
    "        return latest\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding checkpoint: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lWXPzTpgAFlP"
   },
   "outputs": [],
   "source": [
    "def medical_response(patient_input):\n",
    "    prompt = f\"\"\"<|user|>\n",
    "As a doctor, answer medical questions\n",
    "Patient: {patient_input}<|end|>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=False).to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens = 256,\n",
    "        temperature = 0.7,\n",
    "        pad_token_id = tokenizer.eos_token_id,\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True).split(\"<|assistant|>\")[-1].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MRzqHYgiDLJf",
    "outputId": "0e4f9718-85b0-4278-9d94-e7fdf0dd6d53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found checkpoint: /content/drive/MyDrive/medical2_prompt_tuning/checkpoint-1700\n",
      "Latest checkpoint: /content/drive/MyDrive/medical2_prompt_tuning/checkpoint-1700\n"
     ]
    }
   ],
   "source": [
    "latest_checkpoint = find_latest_checkpoint(output_dir)\n",
    "print(f\"Latest checkpoint: {latest_checkpoint}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yWca37exAUai",
    "outputId": "311bc9f2-f931-4928-d817-adada94ebddf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm not a doctor, but I can try to help you understand your symptoms. Fever and body aches can be signs of various conditions, such as the flu, a cold, or even COVID-19. However, it's important to consult a healthcare professional for an accurate diagnosis and treatment. In the meantime, you can try to manage your symptoms with over-the-counter medications like ibuprofen or acetaminophen. Stay hydrated and rest as much as possible. If your symptoms worsen or you have difficulty breathing, seek immediate medical attention.\n"
     ]
    }
   ],
   "source": [
    "print(medical_response(\"I have fever and body aches\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bbOjz3u_AXnW",
    "outputId": "de8bf6bd-bc1b-477a-8467-29d1adbc8aff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the symptoms of fatigue, weight gain, and cold intolerance, some possible differential diagnoses include:\n",
      "\n",
      "1. Hypothyroidism: This condition is characterized by low thyroid hormone levels, which can lead to weight gain, fatigue, and cold intolerance.\n",
      "\n",
      "2. Diabetes Mellitus: Symptoms of diabetes include increased thirst, frequent urination, and weight gain. Fatigue can also be a symptom of uncontrolled diabetes.\n",
      "\n",
      "3. Cushing's Syndrome: This condition is caused by an overproduction of cortisol, a hormone produced by the adrenal glands. Symptoms can include weight gain, fatigue, and cold intolerance.\n",
      "\n",
      "4. Chronic Fatigue Syndrome: This is a complex disorder characterized by extreme fatigue that doesn't go away with rest. Other symptoms can include weakness, muscle pain, and difficulty thinking or concentrating.\n",
      "\n",
      "5. Depression: Depression can cause symptoms such as fatigue, weight gain, and decreased interest in activities.\n",
      "\n",
      "For lab tests, I would recommend:\n",
      "\n",
      "1. Thyroid function tests (TSH\n"
     ]
    }
   ],
   "source": [
    "print(medical_response(\"A 45-year-old male presents with fatigue, weight gain, and cold intolerance. What are the possible differential diagnoses, and which lab tests would you recommend?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bDfsOGnABOOp",
    "outputId": "43606b12-ecfb-4037-d067-42a9f27a53b8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bacterial meningitis and viral meningitis share some similar symptoms, but there are key differences that can help in differentiating between the two. Here are some of the key clinical features:\n",
      "\n",
      "1. Speed of onset: Bacterial meningitis tends to develop more rapidly, often within hours or days, while viral meningitis usually develops over a period of several days.\n",
      "\n",
      "2. Fever: Both conditions can cause fever, but fever is often higher and more severe in bacterial meningitis.\n",
      "\n",
      "3. Headache: Both conditions can cause severe headache, but the headache in bacterial meningitis is often more intense and persistent.\n",
      "\n",
      "4. Neck stiffness: Both conditions can cause neck stiffness, but the neck stiffness in bacterial meningitis is often more pronounced.\n",
      "\n",
      "5. Altered mental status: Both conditions can cause altered mental status, but the alteration in bacterial meningitis is often more severe and can include confusion, delirium, or coma.\n",
      "\n",
      "6. Vomiting: Both conditions can cause vomiting, but vomiting is more common in bacterial meningitis.\n",
      "\n",
      "7. Rash\n"
     ]
    }
   ],
   "source": [
    "print(medical_response(\"What are the key clinical features that differentiate bacterial meningitis from viral meningitis\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lbv0wKMPoVek",
    "outputId": "e488ee48-81af-45d1-841c-4b7d1155a603"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.32.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "n1iXRoTmn7Ll",
    "outputId": "94367fdd-968d-47cc-89f8-f0b83776b7cf"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "RepoUrl('https://huggingface.co/gabbar427/mediguide', endpoint='https://huggingface.co', repo_type='model', repo_id='gabbar427/mediguide')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, HfFolder, create_repo, notebook_login\n",
    "repo_name = \"gabbar427/mediguide\"\n",
    "create_repo(repo_name, repo_type=\"model\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202,
     "referenced_widgets": [
      "4660ff095e4c4f77933dbfe54596f3db",
      "79b12a66545c4bfaaf9c43dc9f3ef774",
      "9a60465c951b4945a4426739d8374ccf",
      "cfb57d612a0a408db6a92209c1da31b2",
      "3f1ce605d6074b7899fed3a1cede24a0",
      "298c3f62f612497197d955f4623a2bd5",
      "ea85ae6484014d6e994d14cc324bda8a",
      "bcc4e2b3fabf470d906019f6cd98cfd5",
      "503f76b074cb44cb98176ed000fa1913",
      "7385f38aa8854c688c0d0d081583cfb1",
      "a1df93f052324db0a81b2538d876f61c",
      "037b80d9a85d4ffbb6fedc553a09da65",
      "bd0d4e5f271d4db5b0d6847497b7948c",
      "1ba2fa9ca4de4c2da5bbce23e315c21a",
      "d83b5a24b76249ce8a6576b8e43d7b3b",
      "0cf03cdbfb214449af1c5a3b9e65f4ea",
      "26dc6cca50004d8da5a241e3a37ef5b2",
      "fc53d9afb7814725847a8d5de320f36d",
      "b5a69cc54c3b41788e3a244b6f015ab6",
      "445a8668f93e406caee9b7feea5ad8ea",
      "5850912cd9c146d0b0e41969e21c71fe",
      "bcd6d0343d3741a2b08436ab4f6a9be4",
      "4a4776a62b644d2999a4d701f8860822",
      "ce39cccb09c445bcbcd198c45e7914d2",
      "01b77727ac6a401187c032d381b9fe21",
      "133944cec1d04d0ba96e9f2c950f5059",
      "57a00351d83048489a1c834e9ad5bcc6",
      "e8208eb5978044fabe8de930b1896bf0",
      "1601532e18264c20bb609ae779b2a28a",
      "06809a5aba864892b95705226ae247e5",
      "3346542cd0d6477f854a6a96dfd37a34",
      "720edde7068b481299a4489b7c8b2e90",
      "6afd72037a7442f587f94ee2712a6076"
     ]
    },
    "id": "ooTV1a5WppKF",
    "outputId": "7cf2b025-ee45-40c6-d67b-5841eadcbaf0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4660ff095e4c4f77933dbfe54596f3db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading...:   0%|          | 0.00/4.14G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "037b80d9a85d4ffbb6fedc553a09da65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a4776a62b644d2999a4d701f8860822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading...:   0%|          | 0.00/587k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/gabbar427/mediguide/commit/3023c574f71c538b7577062370404577b33bef30', commit_message='Upload tokenizer', commit_description='', oid='3023c574f71c538b7577062370404577b33bef30', pr_url=None, repo_url=RepoUrl('https://huggingface.co/gabbar427/mediguide', endpoint='https://huggingface.co', repo_type='model', repo_id='gabbar427/mediguide'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(repo_name)\n",
    "tokenizer.push_to_hub(repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209,
     "referenced_widgets": [
      "047385c9b96f436494995b9bdfb24ee8",
      "69f35d51e2584df5adb5b2d74841a685",
      "715d16a6688d42ff8bd7adb59570830b",
      "4deeab87857940dda0b9dd1287f995d2",
      "72ab1df18a65454fa9b5bbb20d560e50",
      "fc3f921b268f4dd9be83dd11d72d67a6",
      "c40d5681941949bebe68287c5c0f25b5",
      "934e31e06efe4d26a6776a911c29393e",
      "0192f8fa7ff340db96980e01c9680685",
      "e72235d1fc494605b5c81760c893fe33",
      "3bd9cfe55e2a49d6aa7c8b37244701c2",
      "7552d1c7a00b45128d807691f53f8f72",
      "dfaad839dc334783882133b47f6f8848",
      "2359c31816984cc4b8989d4e3452e8fc",
      "f3447e02fc7048d49b69e33e734ebf44",
      "66c4e530d5df45d4a7a996b505f5148b",
      "b6421816e7ba4f20b7791fb700d7d2c9",
      "46e49640087a4586a76d4f612e54bb17",
      "d479ec3cdc7f417585f357a1776a58b1",
      "34ffd16822524d96a29d2377a1746d39",
      "b8ca3d0dfd4941a68e1c0d77f538adea",
      "924fb8d59b2b42d082e45cb0ce82a02e",
      "05acec43f525424d896f283672dfb498",
      "160015e1667440cb88a3a9021e235b4b",
      "8175c9d713de41eb87a711a97b0b0f64",
      "29273fbfbc9c41abba53d4d994fbf0ca",
      "8355a4621be24a589036e56f2a74473b",
      "e9c06baef6ae4ebab58a92e8a74fa7cd",
      "51e73522f33b49ca8e28e2c020798a3e",
      "ef0d6d40e5594908acda8dc63fdb5214",
      "8339e8d56bb1429ba90fb060e66af936",
      "017c021dd10b43ae8e22541f1ab0202b",
      "227ac469aaea4d26abe24d943299db69",
      "58df75daf1a64481b83dda4723d6310b",
      "7eedff3a35d94c0596d3ef62265b1338",
      "f5fd9e39d62040559fe3bcc397c12164",
      "c02162c8ff4a43b0ad58941ac323527e",
      "3e11426d5d284311838d21bc6cff7684",
      "4a44b22d92ee4d0b8f9f31c76aeed307",
      "b3f7843458e143ffad720ad31a3fee84",
      "92e6f26906eb44babbcd5dac5b1b8d86",
      "c95a5390b8ae4dde91437caba47e8f47",
      "a1a62ff4dc0e45d994219148e81891f9",
      "d018697e071a4ddbbd06c1fec8d6fba8",
      "534f1e37450146a3a2af945a3898d2be",
      "68804ce161dc4789953f60cb808a00b1",
      "bdf2425682b5480d8ffd248f0c0f5f54",
      "9816e01920e34e6aafc30faf29ea01c1",
      "0977f2c18d354c9f8e445ed91458039d",
      "71a46ceeb39047ffa421100536a0363e",
      "76835300b9d84eb587fe6ec6ca805b1d",
      "b039915c89e146b09abc693c33b40d9b",
      "8d7ba1fb27df4cdbb0b945c6e15a7577",
      "8f305290a73d4329a64b8fdb54283494",
      "4ecbfa70be7e404b9038ae1fe68aca31",
      "4cdb136fcb774272a6ffa6f346cbe5df",
      "48773217a05147f7859dcaa6ba4e506d",
      "e9ad1b69eaea4b23be5bb69517ee654b",
      "dfc7f9fdbe25460b8982227a1c5617ca",
      "210f7e5ec4574cbd852dfcb1bbcef81f",
      "ee5faf4829474475964258c7472952c5",
      "8a0bac7ca38b46f69b936b3f12ab8348",
      "fa5ef230a2a64dd393e4f29cafbdaf45",
      "dd40ed14c1844bdb8c7187af54047059",
      "89c2b4aced674c5d912937d7513ad3ed",
      "492b40c38ac7470284800acb1287a391"
     ]
    },
    "id": "AvRUVyyRvAqC",
    "outputId": "c386b09e-b03c-42cc-912e-56f9e04a7bf4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "047385c9b96f436494995b9bdfb24ee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/965 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7552d1c7a00b45128d807691f53f8f72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "medicare_110k_train.json:   0%|          | 0.00/126M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05acec43f525424d896f283672dfb498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "medicare_110k_test.json:   0%|          | 0.00/6.60M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58df75daf1a64481b83dda4723d6310b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/106556 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "534f1e37450146a3a2af945a3898d2be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/5609 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cdb136fcb774272a6ffa6f346cbe5df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/5609 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "raw_datasets = load_dataset(\"Mohammed-Altaf/medical-instruction-120k\")\n",
    "test_split = raw_datasets[\"test\"]\n",
    "\n",
    "import re\n",
    "def extract_prompt_response(example):\n",
    "    \"\"\"\n",
    "    Parses the single-string conversation field into:\n",
    "      - instruction: text between “[|Human|]” and “[|AI|]” (or full text if no markers).\n",
    "      - response: text after the last “[|AI|]” marker (or empty if none).\n",
    "    \"\"\"\n",
    "    convo = str(example[next(iter(example.keys()))]).strip()\n",
    "    human_match = re.search(r\"\\[\\|Human\\|\\]\\s*(.*?)\\s*(?=\\[\\|AI\\|\\])\", convo, re.DOTALL)\n",
    "    instruction = human_match.group(1).strip() if human_match else convo\n",
    "    parts = re.split(r\"\\[\\|AI\\|\\]\", convo)\n",
    "    response = parts[-1].strip() if len(parts) > 1 else \"\"\n",
    "    return {\"instruction\": instruction, \"response\": response}\n",
    "\n",
    "test_df = test_split.map(\n",
    "    extract_prompt_response,\n",
    "    remove_columns=test_split.column_names,\n",
    "    num_proc=4\n",
    ")\n",
    "test_prompts = test_df[\"instruction\"]\n",
    "test_references = test_df[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "ZNETZyRVzBWh",
    "outputId": "6a4792d9-4585-4eaa-fab7-c2d3c8986b80"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'I wake in the night, usually about 2-3 hours after going to sleep, with both feet and legs to mid calf feeling like they are on fire. slight red discolorization, minor swelling. This is very painful but after getting up, I can walk it off in about 30 minutes.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prompts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "QihnmmuZzUKY",
    "outputId": "505ccc30-2956-4a38-e41a-b9c6ee063204"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Dear patient Here are the possibilities of what you might have.1)PhlebitisPhlebitis means inflammation of the veins, and can cause redness, itching, irritation, pain, and swelling. A simple Doppler can rule this out.2Blood clot in the lifeblood clots in the leg can become very dangerous, symptoms include swelling, redness, tenderness in the leg. Coagulation profile with an angiography may be required3)Cellulitis: Initial stage. Only can be clinically ruled out Hope this helped'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_references[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IMDS2wMB16fZ",
    "outputId": "06d7f7c9-db65-4efd-b837-3448feadbfc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting test-eval from batch 0.\n",
      " Saved eval state at batch 49 → tokens=102400\n",
      " Saved eval state at batch 99 → tokens=204800\n",
      " Saved eval state at batch 149 → tokens=307200\n",
      " Saved eval state at batch 199 → tokens=409600\n",
      " Saved eval state at batch 249 → tokens=512000\n",
      " Saved eval state at batch 299 → tokens=614400\n",
      " Saved eval state at batch 349 → tokens=716800\n",
      " Saved eval state at batch 399 → tokens=819200\n",
      " Saved eval state at batch 449 → tokens=921600\n",
      " Saved eval state at batch 499 → tokens=1024000\n",
      " Saved eval state at batch 549 → tokens=1126400\n",
      " Saved eval state at batch 599 → tokens=1228800\n",
      " Saved eval state at batch 649 → tokens=1331200\n",
      " Saved eval state at batch 699 → tokens=1433600\n",
      " Saved eval state at batch 749 → tokens=1536000\n",
      " Saved eval state at batch 799 → tokens=1638400\n",
      " Saved eval state at batch 849 → tokens=1740800\n",
      " Saved eval state at batch 899 → tokens=1843200\n",
      " Saved eval state at batch 949 → tokens=1945600\n",
      " Saved eval state at batch 999 → tokens=2048000\n",
      " Saved eval state at batch 1049 → tokens=2150400\n",
      " Saved eval state at batch 1099 → tokens=2252800\n",
      " Saved eval state at batch 1149 → tokens=2355200\n",
      " Saved eval state at batch 1199 → tokens=2457600\n",
      " Saved eval state at batch 1249 → tokens=2560000\n",
      " Saved eval state at batch 1299 → tokens=2662400\n",
      " Saved eval state at batch 1349 → tokens=2764800\n",
      " Saved eval state at batch 1399 → tokens=2867200\n",
      " Saved eval state at batch 1449 → tokens=2969600\n",
      " Saved eval state at batch 1499 → tokens=3072000\n",
      " Saved eval state at batch 1549 → tokens=3174400\n",
      " Saved eval state at batch 1599 → tokens=3276800\n",
      " Saved eval state at batch 1649 → tokens=3379200\n",
      " Saved eval state at batch 1699 → tokens=3481600\n",
      " Saved eval state at batch 1749 → tokens=3584000\n",
      " Saved eval state at batch 1799 → tokens=3686400\n",
      " Saved eval state at batch 1849 → tokens=3788800\n",
      " Saved eval state at batch 1899 → tokens=3891200\n",
      " Saved eval state at batch 1949 → tokens=3993600\n",
      " Saved eval state at batch 1999 → tokens=4096000\n",
      " Saved eval state at batch 2049 → tokens=4198400\n",
      " Saved eval state at batch 2099 → tokens=4300800\n",
      " Saved eval state at batch 2149 → tokens=4403200\n",
      " Saved eval state at batch 2199 → tokens=4505600\n",
      " Saved eval state at batch 2249 → tokens=4608000\n",
      " Saved eval state at batch 2299 → tokens=4710400\n",
      " Saved eval state at batch 2349 → tokens=4812800\n",
      " Saved eval state at batch 2399 → tokens=4915200\n",
      " Saved eval state at batch 2449 → tokens=5017600\n",
      " Saved eval state at batch 2499 → tokens=5120000\n",
      " Saved eval state at batch 2549 → tokens=5222400\n",
      " Saved eval state at batch 2599 → tokens=5324800\n",
      " Saved eval state at batch 2649 → tokens=5427200\n",
      " Saved eval state at batch 2699 → tokens=5529600\n",
      " Saved eval state at batch 2749 → tokens=5632000\n",
      " Saved eval state at batch 2799 → tokens=5734400\n",
      "\n",
      "→ Test complete. Avg. token-loss = 2.7532\n",
      "→ Test Perplexity = 15.69\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# ——————————————————————————————————————\n",
    "#  Compute Test Perplexity with Resume Capability\n",
    "# ——————————————————————————————————————\n",
    "\n",
    "\n",
    "checkpoint_dir = \"/content/drive/MyDrive/medical2_prompt_tuning\"\n",
    "eval_ckpt_path = \"/content/drive/MyDrive/medical2_prompt_tuning/test_eval_state.pth\"\n",
    "batch_size = 2\n",
    "save_every_n_batches = 50\n",
    "\n",
    "\n",
    "class LMTestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=1024):\n",
    "        encodings = tokenizer(\n",
    "            texts,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "        self.input_ids = encodings[\"input_ids\"]\n",
    "        self.attention_mask = encodings[\"attention_mask\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.input_ids.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_mask[idx],\n",
    "            \"labels\": self.input_ids[idx].clone()\n",
    "        }\n",
    "\n",
    "\n",
    "test_texts = [\n",
    "    f\"{instr}\\n\\n{resp}\"\n",
    "    for instr, resp in zip(test_prompts, test_references)\n",
    "]\n",
    "lm_test_dataset = LMTestDataset(test_texts, tokenizer, max_length=1024)\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    lm_test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "device = mistral.device\n",
    "\n",
    "\n",
    "if os.path.isfile(eval_ckpt_path):\n",
    "\n",
    "    state = torch.load(eval_ckpt_path)\n",
    "    start_batch = state[\"last_batch\"] + 1\n",
    "    accumulated_loss = state[\"accumulated_loss\"]\n",
    "    total_tokens = state[\"total_tokens\"]\n",
    "    print(f\"Resuming test-eval from batch {start_batch} (saved on disk).\")\n",
    "else:\n",
    "\n",
    "    start_batch = 0\n",
    "    accumulated_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    print(\"Starting test-eval from batch 0.\")\n",
    "\n",
    "\n",
    "mistral.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(test_loader):\n",
    "        if batch_idx < start_batch:\n",
    "            continue\n",
    "\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        loss = outputs.loss.detach().cpu().item()\n",
    "\n",
    "\n",
    "        nonpad_tokens = (labels != tokenizer.pad_token_id).sum().item()\n",
    "\n",
    "\n",
    "        accumulated_loss += loss * nonpad_tokens\n",
    "        total_tokens += nonpad_tokens\n",
    "\n",
    "\n",
    "        if (batch_idx + 1) % save_every_n_batches == 0:\n",
    "            state = {\n",
    "                \"last_batch\": batch_idx,\n",
    "                \"accumulated_loss\": accumulated_loss,\n",
    "                \"total_tokens\": total_tokens\n",
    "            }\n",
    "            torch.save(state, eval_ckpt_path)\n",
    "            print(f\" Saved eval state at batch {batch_idx} → tokens={total_tokens}\")\n",
    "\n",
    "    final_avg_loss = accumulated_loss / total_tokens\n",
    "    test_perplexity = torch.exp(torch.tensor(final_avg_loss)).item()\n",
    "    os.remove(eval_ckpt_path)\n",
    "\n",
    "    print(f\"\\n→ Test complete. Avg. token-loss = {final_avg_loss:.4f}\")\n",
    "    print(f\"→ Test Perplexity = {test_perplexity:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D7qC1__xQiOq",
    "outputId": "49d8e9bf-ec4a-4a94-f4a0-d4134859fd45"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Latency (per prompt, 128 new tokens): 7.4016 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "# ——————————————————————————————————————\n",
    "#  Measure Average Latency (Generation Time)\n",
    "# ——————————————————————————————————————\n",
    "\n",
    "\n",
    "n_samples = min(50, len(test_prompts))\n",
    "max_new_tokens = 128\n",
    "batch_size = 4\n",
    "\n",
    "\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "latencies = []\n",
    "model.eval()\n",
    "device = model.device\n",
    "\n",
    "\n",
    "dummy_input = tokenizer(\"Hello\", return_tensors=\"pt\").to(device)\n",
    "_ = model.generate(\n",
    "    **dummy_input,\n",
    "    max_new_tokens=10,\n",
    "    do_sample=False,\n",
    "    use_cache=True,\n",
    "    return_dict_in_generate=False\n",
    ")\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "\n",
    "for i in range(0, n_samples, batch_size):\n",
    "    batch_prompts = test_prompts[i : i + batch_size]\n",
    "    inputs = tokenizer(\n",
    "        batch_prompts,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True\n",
    "    ).to(device)\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    _ = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "        return_dict_in_generate=False\n",
    "    )\n",
    "    torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "\n",
    "\n",
    "    elapsed = end - start\n",
    "    latencies.append(elapsed / len(batch_prompts))\n",
    "\n",
    "avg_latency = sum(latencies) / len(latencies)\n",
    "print(f\"Average Latency (per prompt, {max_new_tokens} new tokens): {avg_latency:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vw6krWWnOWeT",
    "outputId": "57e8e327-bed9-4dd3-ed64-e0bd0f4b72c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size on Disk (under “/content/drive/MyDrive/medical2_prompt_tuning/checkpoint-1750”): 5.2 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# ——————————————————————————————————————\n",
    "#  Compute Model Size on Disk\n",
    "# ——————————————————————————————————————\n",
    "\n",
    "def folder_size_in_mb(path: str) -> float:\n",
    "    total_bytes = 0\n",
    "    for root, _, files in os.walk(path):\n",
    "        for fname in files:\n",
    "            fp = os.path.join(root, fname)\n",
    "            total_bytes += os.path.getsize(fp)\n",
    "    return total_bytes / (1024 ** 2)\n",
    "\n",
    "\n",
    "\n",
    "model_size_mb = folder_size_in_mb(latest_checkpoint)\n",
    "print(f\"Model Size on Disk (under “{latest_checkpoint}”): {model_size_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S_C28iieuRnT"
   },
   "outputs": [],
   "source": [
    "# ——————————————————————————————————————\n",
    "# ROUGE Generation with Resume Capability\n",
    "# ——————————————————————————————————————\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "def optimized_medical_generation(model, tokenizer, prompts, references,\n",
    "                                 checkpoint_path, batch_size=8, max_length=512):\n",
    "    \"\"\"FIXED: Efficient generation with resume capability\"\"\"\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        with open(checkpoint_path, 'r') as f:\n",
    "            state = json.load(f)\n",
    "        completed_indices = state.get('completed_indices', [])\n",
    "        predictions = state.get('predictions', [])\n",
    "        print(f\"Resuming from {len(completed_indices)} completed samples\")\n",
    "    else:\n",
    "        completed_indices = []\n",
    "        predictions = []\n",
    "        state = {'completed_indices': completed_indices, 'predictions': predictions}\n",
    "\n",
    "\n",
    "    completed_set = set(completed_indices)\n",
    "\n",
    "\n",
    "    remaining_indices = [i for i in range(len(prompts)) if i not in completed_set]\n",
    "\n",
    "    if not remaining_indices:\n",
    "        print(\"All samples already processed!\")\n",
    "        return predictions, [references[i] for i in completed_indices]\n",
    "\n",
    "    model.eval()\n",
    "    device = model.device\n",
    "\n",
    "\n",
    "    for batch_start in tqdm(range(0, len(remaining_indices), batch_size), desc=\"Medical Generation\"):\n",
    "        batch_end = min(batch_start + batch_size, len(remaining_indices))\n",
    "        batch_indices = remaining_indices[batch_start:batch_end]\n",
    "        batch_prompts = [prompts[i] for i in batch_indices]\n",
    "\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            [f\"MEDICAL PROMPT: {p}\" for p in batch_prompts],  R FORMAT\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=256,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                do_sample=False,\n",
    "                num_beams=1,\n",
    "                use_cache=True,\n",
    "            )\n",
    "\n",
    "\n",
    "        decoded = tokenizer.batch_decode(\n",
    "            outputs,\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "\n",
    "        batch_preds = []\n",
    "        for idx, (prompt, text) in enumerate(zip(batch_prompts, decoded)):\n",
    "\n",
    "            if text.startswith(f\"MEDICAL PROMPT: {prompt}\"):\n",
    "                gen_text = text[len(f\"MEDICAL PROMPT: {prompt}\"):].strip()\n",
    "            else:\n",
    "                gen_text = text.strip()\n",
    "            batch_preds.append(gen_text)\n",
    "\n",
    "\n",
    "        predictions.extend(batch_preds)\n",
    "        completed_indices.extend(batch_indices)\n",
    "\n",
    "\n",
    "        with open(checkpoint_path, 'w') as f:\n",
    "            json.dump({\n",
    "                'completed_indices': completed_indices,\n",
    "                'predictions': predictions\n",
    "            }, f)\n",
    "\n",
    "    return predictions, [references[i] for i in completed_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uW0qedWK44OS"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"/content/drive/MyDrive/rouge_checkpoint.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fwHdSlH3LDtX"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jore5P8A02D1"
   },
   "outputs": [],
   "source": [
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hAkrNEoWLWiz"
   },
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"gabbar427/mediguide\"\n",
    "\n",
    "\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272,
     "referenced_widgets": [
      "81e66ee0d8aa491d923c556e8c751c7a",
      "9d65d260619f448fa4450dc832dfd6d4",
      "b574dd1c85834df6b6cc8ac268b83bc5",
      "e165fd55da8a4ca68e7c02af66b91d30",
      "430f617392e54b25b7ce89fbce395cc8",
      "5725dc8edb634a37844edaf5bb27930a",
      "5f67c78fc3c64c4db5deb0aacfc14090",
      "9c7770e663fc4a4b89320859f74c6626",
      "3f2f2ab7528843bfbce4c56fcbefda7b",
      "33cf02e26a784eda8fbd19027d4a12eb",
      "0871252b1f83411499f92da37b28da53",
      "1dd88e68c3fe4a698d245eb7650878f6",
      "aac3b1171ad74faaaf08452a4e10adfa",
      "26019ea6e7c14314ab3379aa6eb385ba",
      "fb025c15d04948c7854b099613b4ae60",
      "312c951050524a95aede96c557fcbd1c",
      "af8a4cbc54c04e878db51b0077310117",
      "3585ca72dc3f435485d369507af6aa00",
      "b8928fd2908a4bdabd7759030b392d76",
      "a51a02dfc19a49aca7dc01af4e91aa33",
      "b11f09aec02f4f9fbca22055aa0068e2",
      "84a40f61522e46d3b9da8c4504c6ce00",
      "7b596249c98a4af6ae018d4aeb90aee1",
      "8da2ef4940bc45659b0a7b0513c65a69",
      "25405565e7b04f7d8a77947a2481e3cb",
      "6aee0b46432b41688d66195dfc1c9047",
      "96c509ec3ba542fdb3eb06567aa793e9",
      "f99b3ccf06e24586b5750cd1afbf707e",
      "3854ac9eabbb4450a0426028c898201a",
      "0dff6e02231546bb945ff1c7002fc285",
      "32900c7527384f9c81350daefcb442ec",
      "f058e55987614b3b81cde0f41d7a9386",
      "17b397d4e7d84a0a913798225641dc64"
     ]
    },
    "id": "I8uPytihLYFK",
    "outputId": "f00146be-03a7-451f-c59c-32c2c4e01cd5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81e66ee0d8aa491d923c556e8c751c7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.09k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/quantizers/auto.py:222: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dd88e68c3fe4a698d245eb7650878f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.14G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b596249c98a4af6ae018d4aeb90aee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uBAQrnTfLzIj",
    "outputId": "54db96a7-e9b2-4859-b62f-9755a80d17af"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from 2824 completed samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Medical Generation: 100%|██████████| 22/22 [21:55<00:00, 59.80s/it]\n"
     ]
    }
   ],
   "source": [
    "predictions, processed_refs = optimized_medical_generation(\n",
    "        model, tokenizer, test_prompts, test_references, checkpoint_path,\n",
    "        batch_size=8,\n",
    "        max_length=384\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6x6QvvKt0DEw"
   },
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "rouge = load(\"rouge\")\n",
    "results = rouge.compute(\n",
    "        predictions=predictions,\n",
    "        references=processed_refs,\n",
    "        use_stemmer=True,\n",
    "        use_aggregator=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B5V3uiBL0PxH",
    "outputId": "245e639c-7dc9-46d6-d8dd-00888da77ff9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Medical ROUGE Scores:\n",
      "ROUGE-1: 0.2593\n",
      "ROUGE-2: 0.0342\n",
      "ROUGE-L: 0.1310\n",
      "ROUGE-Lsum: 0.1595\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nMedical ROUGE Scores:\")\n",
    "print(f\"ROUGE-1: {results['rouge1']:.4f}\")\n",
    "print(f\"ROUGE-2: {results['rouge2']:.4f}\")\n",
    "print(f\"ROUGE-L: {results['rougeL']:.4f}\")\n",
    "print(f\"ROUGE-Lsum: {results['rougeLsum']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6X7MlYLhaXL5"
   },
   "outputs": [],
   "source": [
    "with open(\"/content/drive/MyDrive/medical_rouge_results.json\", \"w\") as f:\n",
    "        json.dump({\n",
    "            \"scores\": results,\n",
    "            \"predictions\": predictions,\n",
    "            \"references\": processed_refs\n",
    "        }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HEgRmW75PUXr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
