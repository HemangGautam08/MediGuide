{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "asGZvh8WyJBa"
   },
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T05:01:28.693299Z",
     "iopub.status.busy": "2025-06-25T05:01:28.692534Z",
     "iopub.status.idle": "2025-06-25T05:02:46.665346Z",
     "shell.execute_reply": "2025-06-25T05:02:46.664250Z",
     "shell.execute_reply.started": "2025-06-25T05:01:28.693270Z"
    },
    "id": "DrKybbXHQp2o"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install accelerate peft bitsandbytes transformers trl evaluate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T05:02:46.667293Z",
     "iopub.status.busy": "2025-06-25T05:02:46.666952Z",
     "iopub.status.idle": "2025-06-25T05:03:09.353727Z",
     "shell.execute_reply": "2025-06-25T05:03:09.352950Z",
     "shell.execute_reply.started": "2025-06-25T05:02:46.667257Z"
    },
    "id": "68RCKqWPQ8uU",
    "outputId": "a12131d3-5836-4311-ed6d-4e0f443c0750"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-25 05:02:58.947910: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750827779.081006      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750827779.118026      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ROkSKMC2yJBg"
   },
   "source": [
    "# Preparing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_2fPtxS-RAIK",
    "outputId": "55b498ed-cc9c-4576-cc8a-fa093bf6973c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "file_path = '/content/drive/MyDrive/MediGuideDataset/sampled_6000.json'\n",
    "drive_path = \"/content/drive/MyDrive/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ihCBT_9-RHh-",
    "outputId": "3077e8cd-c21c-4aa1-f1ed-7e500a5be7bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found file at /content/drive/MyDrive/MediGuideDataset/sampled_6000.json\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(file_path):\n",
    "    print(f\"Error: File not found at {file_path}\")\n",
    "    print(\"Available files in directory:\")\n",
    "    print(os.listdir(drive_path))\n",
    "else:\n",
    "    print(f\"Found file at {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CFzqvpN3RJnY",
    "outputId": "9605e88b-9e67-4d21-961a-82f141f01ebc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 6000 medical examples\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "try:\n",
    "    with open(file_path) as f:\n",
    "        try:\n",
    "            medical_data = json.load(f)\n",
    "        except json.JSONDecodeError:\n",
    "\n",
    "            content = f.read().split('[file content end]')[0].split('[file content begin]')[-1].strip()\n",
    "            medical_data = json.loads(content)\n",
    "\n",
    "    print(f\"Successfully loaded {len(medical_data)} medical examples\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xnar8sL9Rq5B"
   },
   "outputs": [],
   "source": [
    "def format_data(sample):\n",
    "    instruction = sample.get(\"instruction\", \"\").strip()\n",
    "    input_text = sample.get(\"input\", \"\").strip()\n",
    "    output_text = sample.get(\"output\", \"\").strip()\n",
    "\n",
    "    return {\n",
    "        \"text\": f\"[MED] {instruction}\\nPatient: {input_text}\\nDoctor: {output_text}\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WnPuh_RaRslB"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "dataset = [format_data(d) for d in medical_data]\n",
    "dataset = Dataset.from_list(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6a-GucP9R9Dq"
   },
   "outputs": [],
   "source": [
    "output_dir = \"/content/drive/MyDrive/medical_Adapter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kve2FRpmR-pW"
   },
   "outputs": [],
   "source": [
    "def find_latest_checkpoint(output_dir):\n",
    "    try:\n",
    "\n",
    "        if not os.path.exists(output_dir):\n",
    "            print(f\"Output directory {output_dir} does not exist\")\n",
    "            return None\n",
    "\n",
    "\n",
    "        if not os.listdir(output_dir):\n",
    "            print(f\"Output directory {output_dir} is empty\")\n",
    "            return None\n",
    "\n",
    "\n",
    "        checkpoints = [d for d in os.listdir(output_dir)\n",
    "                      if d.startswith(\"checkpoint\") and os.path.isdir(os.path.join(output_dir, d))]\n",
    "\n",
    "        if not checkpoints:\n",
    "            print(\"No checkpoint directories found\")\n",
    "            return None\n",
    "\n",
    "\n",
    "        checkpoints.sort(key=lambda x: int(x.split(\"-\")[1]))\n",
    "        latest = os.path.join(output_dir, checkpoints[-1])\n",
    "        print(f\"Found checkpoint: {latest}\")\n",
    "        return latest\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding checkpoint: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oKUqpZMvSAFm",
    "outputId": "a8ece45d-fa90-4ed1-86ba-3ba640fb8892"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory /content/drive/MyDrive/medical_Adapter is empty\n",
      "Latest checkpoint: None\n"
     ]
    }
   ],
   "source": [
    "latest_checkpoint = find_latest_checkpoint(output_dir)\n",
    "print(f\"Latest checkpoint: {latest_checkpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mOfxy9I2yJBl"
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T05:14:55.208091Z",
     "iopub.status.busy": "2025-06-25T05:14:55.207397Z",
     "iopub.status.idle": "2025-06-25T05:14:55.301687Z",
     "shell.execute_reply": "2025-06-25T05:14:55.301118Z",
     "shell.execute_reply.started": "2025-06-25T05:14:55.208068Z"
    },
    "id": "mRxS2qLcSC8d"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"YOUR HF TOKEN HERE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T05:03:49.847349Z",
     "iopub.status.busy": "2025-06-25T05:03:49.846819Z",
     "iopub.status.idle": "2025-06-25T05:03:49.850824Z",
     "shell.execute_reply": "2025-06-25T05:03:49.850174Z",
     "shell.execute_reply.started": "2025-06-25T05:03:49.847322Z"
    },
    "id": "bQTJRZBGa0MS"
   },
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "55ada32045fa4b8598eb6d6ab3340fb6",
      "002cdfc6ec82466c9f3786a2d023f85a",
      "e9690075c9ce4f5b9daf719ebab65479",
      "78ff5b495d1c494fa3047ff8cf71f5b6",
      "ab48cb9c99fc47b390c5d307479c4850",
      "d1751e11d3734349a876235f8fd45b13",
      "7ae588e6236b4fbea14c52e51f3fe9d2",
      "3eb1d4f5ed18415ba398fbcc151ab84d",
      "e2b3ed2d41e145c6a5e2e19edca5102b",
      "c830c05f2d194200b7f12a6e4e55f0e7",
      "effaa2744cca442a9603dfddf6b053db"
     ]
    },
    "id": "DEQg5rMfSC5E",
    "outputId": "430fdf97-5f02-4c5f-c960-caaf357afb5b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55ada32045fa4b8598eb6d6ab3340fb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    offload_folder=\"./offload\",\n",
    "    offload_state_dict=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lQBEOGzqX1kV"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from peft import get_peft_model, PrefixTuningConfig, TaskType\n",
    "from datasets import load_dataset, Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EVOV88ScSFBr",
    "outputId": "a091a261-f0ea-42e5-f154-51c307b989be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 272,719,872 || all params: 7,520,743,424 || trainable%: 3.6262\n"
     ]
    }
   ],
   "source": [
    "# Setup Prefix Tuning ----\n",
    "peft_config = PrefixTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    num_virtual_tokens=20,\n",
    "    encoder_hidden_size=model.config.hidden_size,\n",
    "    prefix_projection=True\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "633a6d1a49ec41f098c164f5f952cdc9",
      "fde2803c2bd141028187b8fdfd1625db",
      "e90b79b439b24d3c995932332c4adce9",
      "8fc46e67a78f4202bf6573e73a3a8743",
      "908751e78adc40678ec71ec661366da0",
      "91076b7fdaf843719d6c5fd57f5b0eee",
      "7883d4aa354d4df39dfd88625601db3e",
      "7afeca44cadc4a039ce3f64e27d59566",
      "7e4049deed9141c196d71fd86edf72bf",
      "05a8de25c36b4dbeaeabd930b09d954c",
      "bfa33956fe4c4560899b67a457c01c0c"
     ]
    },
    "id": "mG54U7osXyam",
    "outputId": "0dd01360-9f42-4e6e-ac94-e6a94b6c23da"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "633a6d1a49ec41f098c164f5f952cdc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenization ----\n",
    "def tokenize(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C3p2uv4XY2pX",
    "outputId": "61d9742f-9f39-431d-a4c7-c4a7af2da9ce"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-18-793372679.py:18: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "#  Training Setup ----\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=5e-5,\n",
    "    logging_steps=1000,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    # bf16=True,\n",
    "    fp16=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fRuij8dBcmdW",
    "outputId": "3848765c-a6ea-4ea0-f513-2690ba454626"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): MistralForCausalLM(\n",
       "    (model): MistralModel(\n",
       "      (embed_tokens): Embedding(32768, 4096)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x MistralDecoderLayer(\n",
       "          (self_attn): MistralAttention(\n",
       "            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (mlp): MistralMLP(\n",
       "            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "      (rotary_emb): MistralRotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=4096, out_features=32768, bias=False)\n",
       "  )\n",
       "  (prompt_encoder): ModuleDict(\n",
       "    (default): PrefixEncoder(\n",
       "      (embedding): Embedding(20, 1024)\n",
       "      (transform): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (1): Tanh()\n",
       "        (2): Linear(in_features=4096, out_features=65536, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (word_embeddings): Embedding(32768, 4096)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 262
    },
    "id": "Bgav6SugZWYc",
    "outputId": "a4b0346a-9ab5-4ac3-bd53-7b86eb1b3f1c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='807' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 807/3000 31:31 < 1:25:53, 0.43 it/s, Epoch 0.27/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3000/3000 1:55:54, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.120700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.015300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.995500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3000, training_loss=2.043830485026042, metrics={'train_runtime': 6958.1293, 'train_samples_per_second': 0.862, 'train_steps_per_second': 0.431, 'total_flos': 1.31121668947968e+17, 'train_loss': 2.043830485026042, 'epoch': 1.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train ----\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ktgvk0NjfG2s"
   },
   "outputs": [],
   "source": [
    "# Save PEFT adapter ----\n",
    "model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yEq53Hix45mX",
    "outputId": "3b1d0add-6faa-43de-8323-640d3bbe6193"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/content/drive/MyDrive/medical_Adapter/tokenizer_config.json',\n",
       " '/content/drive/MyDrive/medical_Adapter/special_tokens_map.json',\n",
       " '/content/drive/MyDrive/medical_Adapter/chat_template.jinja',\n",
       " '/content/drive/MyDrive/medical_Adapter/tokenizer.model',\n",
       " '/content/drive/MyDrive/medical_Adapter/added_tokens.json',\n",
       " '/content/drive/MyDrive/medical_Adapter/tokenizer.json')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N0GlWvWm5a83"
   },
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19fbji6lyJBr"
   },
   "source": [
    "# Uploading to HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "1b06c10451114d79911ff5292ccc3606"
     ]
    },
    "execution": {
     "iopub.execute_input": "2025-06-25T05:26:03.450276Z",
     "iopub.status.busy": "2025-06-25T05:26:03.449918Z",
     "iopub.status.idle": "2025-06-25T05:26:03.465815Z",
     "shell.execute_reply": "2025-06-25T05:26:03.464927Z",
     "shell.execute_reply.started": "2025-06-25T05:26:03.450250Z"
    },
    "id": "rZTFsSqHyJBr",
    "outputId": "e2c18a39-bc5a-46b9-af16-6ba27eaa175e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b06c10451114d79911ff5292ccc3606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "ecfe5a9392bd4043becfe60656694817",
      "045b5cabd5254859a578ba7cfd4907d4",
      "ff99096efe674fb7b9d958ebc552a213"
     ]
    },
    "execution": {
     "iopub.execute_input": "2025-06-25T05:26:16.051587Z",
     "iopub.status.busy": "2025-06-25T05:26:16.050872Z",
     "iopub.status.idle": "2025-06-25T05:26:19.185919Z",
     "shell.execute_reply": "2025-06-25T05:26:19.185264Z",
     "shell.execute_reply.started": "2025-06-25T05:26:16.051552Z"
    },
    "id": "DiMa6yzo6PPs",
    "outputId": "ea6aa47b-c63a-42eb-d69e-0056aa012828"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecfe5a9392bd4043becfe60656694817",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/5.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "045b5cabd5254859a578ba7cfd4907d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff99096efe674fb7b9d958ebc552a213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/ankraj/mediguide/commit/6bec6a650fbb561b8fa3c42be30e52404268a445', commit_message='Upload tokenizer', commit_description='', oid='6bec6a650fbb561b8fa3c42be30e52404268a445', pr_url=None, repo_url=RepoUrl('https://huggingface.co/ankraj/mediguide', endpoint='https://huggingface.co', repo_type='model', repo_id='ankraj/mediguide'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login, create_repo\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "# Step 2: Create repo (only once)\n",
    "repo_name = \"ankraj/mediguide\"\n",
    "create_repo(repo_name, repo_type=\"model\", exist_ok=True)\n",
    "\n",
    "# Step 3: Push model and tokenizer\n",
    "model.push_to_hub(repo_name)\n",
    "tokenizer.push_to_hub(repo_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pzMQTqZysT5"
   },
   "source": [
    "# Importing Model from HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EKcl7JZ22cT0"
   },
   "outputs": [],
   "source": [
    "path = \"ankraj/mediguide\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_CUqUrEjySN6",
    "outputId": "a0b6ecf5-df44-4055-e13b-26ee619a39aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers accelerate bitsandbytes\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465,
     "referenced_widgets": [
      "56d09803c2244dc98b4eec7336044cc6",
      "d3358b5c5f8e412fa000222b419c7d16",
      "1a8a90701bfa47f98149f6a767766273",
      "6b3317d295b14eb2a7fa37ab1e88747d",
      "85dd0a94bff44bc9bf36a45a80b1638b",
      "d0ea75a3c7d44893ac37b5b6451db076",
      "20d7aa41e3fc49339da320a2c6b65fa5",
      "a03934b95dd84386be16d024c06a31b1",
      "68b82a8bdca544a8a99f21e434781a6a",
      "f523c178755c43b491b366beda2c89b4",
      "4002d476365b4c2d99a6ba3c2dee145f",
      "a29f897f984549b2aa32d7ade4fefc5f",
      "0c71e68c12aa4abf9f8a616474678fa9",
      "ab43c908cbf54822a2e310ce194052c5",
      "c4ac993ec7954a299842aafdbe1f9af9",
      "22ea60bfd4e94ca79edff3e888ad5246",
      "d3716099f371412487321daebe7397fe",
      "b0b9def5c7b84fe193ebfe1bdba75a21",
      "d7dc403b80c94dd0bc59b50d34c46859",
      "9e4233200d1546f5ae8a86d40e3e9b03",
      "6ab5bfaddd8046d38a7685064fe6f69c",
      "44bf384cd16b4f09a5d765b7d064b976",
      "917362511b0b4637bb022a7650bae91a",
      "0098ebe5067347cb9c7a7eb9c2e4f96d",
      "8f63ddb74ca246ed9d6970f63ad73fa8",
      "0a0dc8ce601545b0ac71437072b9611e",
      "f465a0eb3c8442abb9f1667950091779",
      "367e1802906248d8899e44e75eceebda",
      "bbf330a6965b49a290aea5402d1de6a5",
      "37a88a427660475f995cf79c5bb7ef81",
      "712170713bb4481d95a0c5ad1667017d",
      "9d4aa064203c41a6be88f9467972fa76",
      "298387f0a39e44a89c4dfd5b7c1ca081",
      "a6e5d4c09e0841be8a5413c92d7e7be7",
      "616229a2b5ff44aa9f13eee6f4d210a2",
      "be59d3146cc04f7882989a5b4f1d8afa",
      "bb52f5cb472444eb86bde8645396db93",
      "0798b09ccdf443b4b9dc3defeda59e3a",
      "a12a93330aec446aabb781b54b9d8161",
      "a40cb690854c4129bbee3dbcc0e42493",
      "7ac170054f5f4bd2ba1dc15fe4082cbb",
      "a7572afb963d40bca1f2cdde89786d6d",
      "f1d0c12c91ca4afb9cde664270c42484",
      "0c20fde0a5b5400aafe6ed86d0cb888d",
      "d29f76153ead46f0b6ac58401d88ba1c",
      "a6af8fbb2c6c47f9bbb62df2e1c025f2",
      "f84323797d8d457098e3adf660e26968",
      "66f460b08c9046eb97e46beb7d05a710",
      "9e2f3c4e33434006a99ca00a381e6d18",
      "a8328315a0164435b800b4fddea40c6b",
      "7c4c7aff0a1c4256aa8f436f6f7f57b8",
      "085f2b7119ee490ab6ad2b299b1ab284",
      "e6d61ed4d2a44bfb8f5377838d3ba1a1",
      "0f313163d70a44b6acebc17bc0f7ec5b",
      "a55101f841f843ffbeb5262892604b84",
      "fa5183372f3342729cbe8c07399270bc",
      "eb671416110c49249aadc931250c7d7d",
      "24e2d6d94acf4860b0df92e54b985a8b",
      "2c898d3288ef432db413cd873d1209b1",
      "b4c1231f5d6e47498d6f52b8899cbbcc",
      "28b607e7a98d4f4fb65e8bbf78086db2",
      "b4dd9813c42d4ed7b617a91a978b4a2d",
      "31e5c6381b084a5cb94f433cd7b4497f",
      "9a496e29141d4ab58e9871758491b999",
      "870123016aae42aa9a181df1c8b3d160",
      "9e11292a0bc44f40b8cb76c136a38e27",
      "47ffd53fd8d84c1282cb9a18db883c5c",
      "a6e2a37eea094c2290dd7adfcd34f64c",
      "44bf7bd848ce4eb58fd9a2c03a8b4d8e",
      "538ba425cd6148d6a7b26f38c88ad2bc",
      "8245d4a65b8d44ffb06d00e03482a3fa",
      "ade7dabe03194100b7e309ac823d7d84",
      "cbe57441bd534efd98450ab83e84c70c",
      "7a6cee4c49934438839fdf850345c58c",
      "298ac7bc3f8b4048832764a2fb934544",
      "605be9c995214b0cbe46432e1fb7dbc7",
      "64009a70b9474b5dac9c50520ee1763d",
      "37c34a48c079429cbeead5ee6132c604",
      "626a90f362f5439686407d533aebd4f1",
      "067a69b0697b418fa07224233718876c",
      "22f9ca1ac04b488aa4332eaf4724d535",
      "de897e586d7e46c99d9a160c40e3f77e",
      "a0f0450f539f4f94b7df8758773d0b97",
      "1adcaf368ce7445ca2e859a4e14299c0",
      "32c0ef44986b494d9ea718603e33c3f9",
      "9d4daa471a7d40f989032f40316bbbc2",
      "eb400419866540c9931c42dc21a2e83a",
      "bb6a6443114e496a89af59df8ac61581",
      "3cf760ac778e4b59972b01997677490d",
      "671c4d018f16485099585e0553f4ed41",
      "9aee8a55215a4b5ba32edd84ad6ce005",
      "2d4683aac5764de4b49554eb8339621a",
      "d5df4137e5cf4b39b0209c89463d9914",
      "d3f3eec7246445959b8a722086969098",
      "3bf4a4f7f18445efbff84eba1b996bdf",
      "8df95b73c7fe46e2ab42372632070e48",
      "e4044c70acb249b9a1c265c93254b766",
      "65dabd9b1eda4186801ad92a16b58a6c",
      "ca2e47ce9e0945c9bc79d4bed2d001f4",
      "7766492071f24bc2a6ac447bb7936e05",
      "e68bba39616f4843856475e4d4a48356",
      "fe4dc3d3d9d4417490ca41abcc003369",
      "bccabc987f1446bdba0d1acd3d8ce89e",
      "8af5a43712ff4cc6b8319d16404f81b5",
      "3351a9360d3a4884954d40f428716660",
      "91987586ed314167bbf5d57c5508e6ef",
      "3178cf69f3e54119b497d64ed25142fa",
      "aa53c64b4a6547bbb2e0b75a5e096418",
      "ab6482f7a45a4948b1ed870adf24137e",
      "8f697d45697648a4b7f49642100495b6",
      "2183cc411939444c9cee6f82759d6d45",
      "f7f7944ed70c45628334f77ba978252b",
      "5c20010970e64d3c88a472b4119d8689",
      "b34efc1676af4b2eb8be2ccd4ecc14fa",
      "3decb55f9c7448fb82fcd3a5bedf360e",
      "2001084b5fb94d03871b4491aedbbfbd",
      "37c2436dc3004622a82a07ad6be228eb",
      "07f680b547084b9295ee4cfc2a0cc855",
      "ed07df25520f418ebe5b859325fae7f2",
      "9ea5803e646d4516be772783727a6811",
      "15effdf2e3f941e0bb26e28e855a1e77",
      "f3e9872761bf435d8893ce23cf6c2aac",
      "16a63e2df19c4274bcb55dd4238a0e92",
      "6a8f9bb585ea41089c176381acb55e97",
      "d44afa1a51e24dd18503aa6c4930fcb0",
      "49dfe84bbda2414d8c7b195742450997",
      "1ea949f607204c92b8dc463f37d64f66",
      "e7867f3c3f124a87a3aace6c9baaef92",
      "c911e669adea42cfb2dc0d14c3d76af6",
      "dba28076259341c5b7f90f669aad5d2d",
      "5d06fc91bdbc4b17addb5cc8d168fdae",
      "067b6a3ee7d648c299318dab6d558554",
      "4105c5821263487081167646be1c123f",
      "96aea197ddd54d9fb84dbbde5f4eb69a",
      "dee6edfbbecb4c34853202c55a030809",
      "d7f18bcb1cb24d8b909cfc9d8474fa10",
      "6ce8e7a58b3d4617a1635ca6fc8b3dbf",
      "036b0828a2294ff78224194bf7a3d7e8",
      "ef238062389b4e4780cac1654e31692f",
      "9132779eff094f0ea61533f864e35ca4",
      "46fc8e62b44a4be89a8d09ecb410bc0c",
      "e5f1ac6f5c1c43c4947873fa47a01183",
      "b7d1bc1d66d34fa5a0051e9d44676000",
      "ef3daf1d55544775b21f94ce9b3e9030",
      "109a7564407c45dc843eef130704108b",
      "fccf986564b440a1b93019eebd2e32cb",
      "01be32c846934872a01bce518a2aa9a4",
      "56f31de0023d460eadd6f4e4e458cd13",
      "907bc4ea469e4b1c90479fbc8a0e7996",
      "d2609acb9f9645509811b45ccac02698",
      "a1eadf9ca58e4da4912580885efc0aef",
      "c9baef5f92394a3aa980cffe10f5cb5c",
      "ac21d99e8b3a404489fdf954d371ada0",
      "41301f422fb548209937ccc442250fb0"
     ]
    },
    "id": "gnGRanC2yRy9",
    "outputId": "6c04e6d0-0e10-4c58-f2e9-4ead54e6ddec"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56d09803c2244dc98b4eec7336044cc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/601 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a29f897f984549b2aa32d7ade4fefc5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "917362511b0b4637bb022a7650bae91a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6e5d4c09e0841be8a5413c92d7e7be7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d29f76153ead46f0b6ac58401d88ba1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.55G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa5183372f3342729cbe8c07399270bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47ffd53fd8d84c1282cb9a18db883c5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37c34a48c079429cbeead5ee6132c604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cf760ac778e4b59972b01997677490d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/391 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7766492071f24bc2a6ac447bb7936e05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/5.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2183cc411939444c9cee6f82759d6d45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3e9872761bf435d8893ce23cf6c2aac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4105c5821263487081167646be1c123f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef3daf1d55544775b21f94ce9b3e9030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/437 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Load base model first\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    offload_folder=\"./offload\",\n",
    "    offload_state_dict=True\n",
    ")\n",
    "\n",
    "# Inject adapter\n",
    "model = PeftModel.from_pretrained(base_model, path)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "uFRVHxZzEIOK",
    "outputId": "fe9cc209-13a9-45e6-93f2-ced9b41d9828"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): MistralForCausalLM(\n",
       "    (model): MistralModel(\n",
       "      (embed_tokens): Embedding(32768, 4096)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x MistralDecoderLayer(\n",
       "          (self_attn): MistralAttention(\n",
       "            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (mlp): MistralMLP(\n",
       "            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "      (rotary_emb): MistralRotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=4096, out_features=32768, bias=False)\n",
       "  )\n",
       "  (prompt_encoder): ModuleDict(\n",
       "    (default): PrefixEncoder(\n",
       "      (embedding): Embedding(20, 65536)\n",
       "    )\n",
       "  )\n",
       "  (word_embeddings): Embedding(32768, 4096)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Wb_I2P48700I"
   },
   "outputs": [],
   "source": [
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "AWV97fT8Dm1S"
   },
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __init__(self, stop_phrases, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stop_ids_list = [\n",
    "            tokenizer(phrase, return_tensors=\"pt\").input_ids[0][1:]  # remove BOS\n",
    "            for phrase in stop_phrases\n",
    "        ]\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        device = input_ids.device\n",
    "        for stop_ids in self.stop_ids_list:\n",
    "            stop_ids = stop_ids.to(device)  # ✅ Move to same device\n",
    "            if len(input_ids[0]) >= len(stop_ids):\n",
    "                if torch.equal(input_ids[0][-len(stop_ids):], stop_ids):\n",
    "                    return True\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "1cLn4pS43-Uf"
   },
   "outputs": [],
   "source": [
    "def preprocess_input(input_text):\n",
    "    instruction = \"If you are a doctor, please answer the medical questions based on the patient's description.\"\n",
    "    prompt = f\"[MED] {instruction}\\nPatient: {input_text} \\nDoctor:\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "uqK59j2F4GPR"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_output(text):\n",
    "    stop_patterns = [\n",
    "        r\"Take care Chat Doctor\\.\",\n",
    "        r\"Regards, Chat Doctor\\.\",\n",
    "        r\"Regards. Chat Doctor\\.\",\n",
    "        r\"Wishing you good health\\.\",\n",
    "        r\"Goodbye\\.\",\n",
    "        r\"Take care\\.\",\n",
    "        r\"\\.com\"\n",
    "    ]\n",
    "\n",
    "    doc_match = re.search(r\"Doctor:\\s*(.*)\", text, re.DOTALL | re.IGNORECASE)\n",
    "    if not doc_match:\n",
    "        return text.strip()\n",
    "\n",
    "    after_doctor = doc_match.group(1)\n",
    "\n",
    "    stop_pattern = r\"(.*?)(\" + \"|\".join(stop_patterns) + \")\"\n",
    "    stop_match = re.search(stop_pattern, after_doctor, re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "    if stop_match:\n",
    "        return stop_match.group(1).strip() + \" \" + stop_match.group(2)\n",
    "\n",
    "    return after_doctor.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "LUXA2eS9Dxlr"
   },
   "outputs": [],
   "source": [
    "def run_medical_bot(input_text, max_new_tokens=500):\n",
    "    prompt = preprocess_input(input_text)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # List of phrases that should stop generation\n",
    "    stop_phrases = [\n",
    "        \"Take care Chat Doctor.\",\n",
    "        \"Regards, Chat Doctor.\",\n",
    "        \"Regards. Chat Doctor.\",\n",
    "        \"Wishing you good health.\",\n",
    "        \"Goodbye.\",\n",
    "        \"Take care.\"\n",
    "    ]\n",
    "\n",
    "    stopping_criteria = StoppingCriteriaList([StopOnTokens(stop_phrases, tokenizer)])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            stopping_criteria=stopping_criteria\n",
    "        )\n",
    "\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return clean_output(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-dIXu53d4Zmd"
   },
   "outputs": [],
   "source": [
    "input_text = \"I am feeling uneazy.. I have vomitted 3 times in the last 2 year. I am 26 female having no prior such health condition.\"\n",
    "output = run_medical_bot(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209
    },
    "id": "v-NOhdYLDy70",
    "outputId": "a84e0582-d1ee-4183-8c97-519f845ae24d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"Thanks for your question on Chat Doctor. I can understand your concern. By your history, your symptoms are suggestive of recurrent episodes of vomiting. You are also having symptoms of bloating and weight gain. So my first advice to you is to consult gastroenterologist and get done clinical examination of abdomen. He will also advise you for investigations like blood test, urine test, stool test, ultrasound abdomen, CT scan abdomen etc. For vomiting, you should start treatment with proton pump inhibitor (PPI) like esomeprazole. Take it once in the morning. Don't forget to take it. You can also take antiemetic (anti-vomiting) Chat Doctor.  If you are still having vomiting, then you should take prokinetic like metoclopramide. It is also very effective in controlling vomiting. Take it in the morning. Don't forget to take it. Also take antacid like omeprazole once in the night. Don't forget to take it. If you are still having symptoms of bloating, then you should take alosetron. It is very effective in bloating. Take it once in the morning. Don't forget to take it. If you are still having symptoms of weight gain, then you should take slimming tablets like orlistat. Take it once in the morning. Don't forget to take it. This is the treatment you should follow. Please follow all the advice given here. You should also follow healthy diet and regular exercise. Avoid junk food, Chat Doctor.  Avoid stress and tension. Avoid smoking and alcohol. Don't worry, you will be alright. You can discuss any details with me. I will be happy to help you. I wish you good health and quick recovery. Please remember that I am not your treating doctor. So I am not having all the details of your case. So my advice may not be suitable for you. So you should discuss everything with your doctor and follow his advice. Hope I have answered your question. Wish you good health. Thanks. Regards. Chat Doctor.\""
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FlIxHDZ8Dy1q",
    "outputId": "44ce3dca-d7f7-4b0a-83b8-2aa13601e5d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, Welcome to Chat Doctor, I have gone through your query and understand your concern. The injury could be a fracture, dislocation, or a muscle tear. I would advise you to get an X-ray and MRI done. If there is a fracture, it can be treated with a sling. If there is a dislocation, it should be reduced under anesthesia. If there is a muscle tear, it can be treated with a sling and physiotherapy. You should rest the arm as much as possible and avoid any strenuous activities. You should also avoid lifting heavy objects. Take a painkiller to help with the pain. I would advise you to apply a cold pack to the affected area. You should also elevate the arm above the level of the heart. This will help to reduce the swelling. I would also advise you to take an antispasmodic to help with the pain. You should also take a muscle relaxant to help with the pain. You should also take a multivitamin and a calcium supplement to help with the healing process. I would advise you to consult an orthopedic surgeon for further management. You should also follow the treatment advised by your doctor. Hope this answers your query. If you have additional questions or follow-up queries then please do not hesitate in writing to us. I will be happy to answer your queries. Wishing you good health.\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Hello, At the end of lacrosse practice about a week ago i recieved a nasty cross check to my deltoid. The check hit wierd, as it went under my pad. The pain came in right away, couldnt move my arm for the rest of the night. I was surprised to see that that there was a very small bruise , but my whole shoulder hurts to the point where i cannot do simple tasks such as passing the ball. I can slowly move my arm fine, but when i speed things up it stings all over. I have bern icing it every day. I have been on advil only to help with the pain, but is there anything else i can do? Do you know what could be wrong? Any methods of treating it faster? I need to get back on the field asap before try outs are over\"\n",
    "output = run_medical_bot(input_text)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r0r87zkJHrMZ"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A1efMV3iHpiR",
    "outputId": "756ab9fd-27e5-49d3-9724-729884830810"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge-score --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xz0iSWW3K243",
    "outputId": "b7e8884f-f8d3-489c-997b-e4a4a472cdda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WIDsEer8MmpZ",
    "outputId": "03cd75dd-26bb-4b75-a43c-c4a85e1903c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5609 examples.\n",
      "Dataset({\n",
      "    features: ['Conversation'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "{'Conversation': 'The conversation between human and AI assistant.\\n[|Human|] I wake in the night, usually about 2-3 hours after going to sleep, with both feet and legs to mid calf feeling like they are on fire. slight red discolorization, minor swelling. This is very painful but after getting up, I can walk it off in about 30 minutes.\\n[|AI|]  Dear patient Here are the possibilities of what you might have.1)PhlebitisPhlebitis means inflammation of the veins, and can cause redness, itching, irritation, pain, and swelling. A simple Doppler can rule this out.2Blood clot in the lifeblood clots in the leg can become very dangerous, symptoms include swelling, redness, tenderness in the leg. Coagulation profile with an angiography may be required3)Cellulitis: Initial stage. Only can be clinically ruled out Hope this helped\\n'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "# Read your JSONL file manually\n",
    "data = []\n",
    "with open(\"/content/drive/MyDrive/MediGuideDataset/medicare_110k_test.json\", \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            data.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(data)} examples.\")\n",
    "\n",
    "# Convert directly to a plain Dataset object (not DatasetDict!)\n",
    "dataset = Dataset.from_list(data)\n",
    "\n",
    "# OPTIONAL: If you only want first 1000 examples\n",
    "test_split = dataset.select(range(1000))\n",
    "\n",
    "print(test_split)\n",
    "print(test_split[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138,
     "referenced_widgets": [
      "bfc550bce5614a78b15884f899351128",
      "ed2af637a75d4f0b9b1e7525c4310ca8",
      "a31c6d93f2334a7ca18fe9c2f62de9cf",
      "5302bd6369da452f80e13b89a6dcb6f4",
      "eb055fa447354320bdeb5c5e576c58dc",
      "29f6f90874194437855cd5a23cb14655",
      "301c9277715d4229b682ae6d0901f942",
      "25440b3fbecc4dccb9a36de3cb332c4e",
      "3073cf2f050041609086a892d4ca2c12",
      "29905cb6ced540dfae8ce030dcf7e965",
      "cf0ce00b082f45b18b78f3168d06f9a5"
     ]
    },
    "id": "8dMzGDxbM87m",
    "outputId": "9157509b-b9ff-40b1-b43d-175c8bbd6ba2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfc550bce5614a78b15884f899351128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/usr/local/lib/python3.11/dist-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def extract_prompt_response(example):\n",
    "    \"\"\"\n",
    "    Parses the single-string conversation field into:\n",
    "      - instruction: text between “[|Human|]” and “[|AI|]” (or full text if no markers).\n",
    "      - response: text after the last “[|AI|]” marker (or empty if none).\n",
    "    \"\"\"\n",
    "    convo = str(example[next(iter(example.keys()))]).strip()\n",
    "    human_match = re.search(r\"\\[\\|Human\\|\\]\\s*(.*?)\\s*(?=\\[\\|AI\\|\\])\", convo, re.DOTALL)\n",
    "    instruction = human_match.group(1).strip() if human_match else convo\n",
    "    parts = re.split(r\"\\[\\|AI\\|\\]\", convo)\n",
    "    response = parts[-1].strip() if len(parts) > 1 else \"\"\n",
    "    return {\"instruction\": instruction, \"response\": response}\n",
    "\n",
    "test_df = test_split.map(\n",
    "    extract_prompt_response,\n",
    "    remove_columns=test_split.column_names,\n",
    "    num_proc=4\n",
    ")\n",
    "test_prompts = test_df[\"instruction\"]\n",
    "test_references = test_df[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "uIP_puujNNwJ",
    "outputId": "7472a938-a7f2-421c-af0a-fbf894102a62"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'I wake in the night, usually about 2-3 hours after going to sleep, with both feet and legs to mid calf feeling like they are on fire. slight red discolorization, minor swelling. This is very painful but after getting up, I can walk it off in about 30 minutes.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prompts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "E5cuhCeyNQrf",
    "outputId": "9509e85d-bb9e-4b3a-f0fe-259f1e1a7a4f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Dear patient Here are the possibilities of what you might have.1)PhlebitisPhlebitis means inflammation of the veins, and can cause redness, itching, irritation, pain, and swelling. A simple Doppler can rule this out.2Blood clot in the lifeblood clots in the leg can become very dangerous, symptoms include swelling, redness, tenderness in the leg. Coagulation profile with an angiography may be required3)Cellulitis: Initial stage. Only can be clinically ruled out Hope this helped'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_references[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y1CXRXBbNUci",
    "outputId": "be333624-71b5-42da-84c6-09ce408666eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting test-eval from batch 0.\n",
      " Saved eval state at batch 49 → tokens=102400\n",
      " Saved eval state at batch 99 → tokens=204800\n",
      " Saved eval state at batch 149 → tokens=307200\n",
      " Saved eval state at batch 199 → tokens=409600\n",
      " Saved eval state at batch 249 → tokens=512000\n",
      " Saved eval state at batch 299 → tokens=614400\n",
      " Saved eval state at batch 349 → tokens=716800\n",
      " Saved eval state at batch 399 → tokens=819200\n",
      " Saved eval state at batch 449 → tokens=921600\n",
      " Saved eval state at batch 499 → tokens=1024000\n",
      "\n",
      "→ Test complete. Avg. token-loss = 5.8102\n",
      "→ Test Perplexity = 333.69\n"
     ]
    }
   ],
   "source": [
    "# 1. PERPLEXITY\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "import os\n",
    "\n",
    "checkpoint_dir = \"/content/drive/MyDrive/medical_prefix\"\n",
    "eval_ckpt_path = \"/content/drive/MyDrive/medical_prefix/test_eval_state.pth\"\n",
    "batch_size = 2\n",
    "save_every_n_batches = 50\n",
    "\n",
    "\n",
    "class LMTestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=1024):\n",
    "        encodings = tokenizer(\n",
    "            texts,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "        self.input_ids = encodings[\"input_ids\"]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.input_ids.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"labels\": self.input_ids[idx].clone()\n",
    "        }\n",
    "\n",
    "\n",
    "test_texts = [\n",
    "    f\"{instr}\\n\\n{resp}\"\n",
    "    for instr, resp in zip(test_prompts, test_references)\n",
    "]\n",
    "lm_test_dataset = LMTestDataset(test_texts, tokenizer, max_length=1024)\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    lm_test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "device = model.device\n",
    "\n",
    "\n",
    "if os.path.isfile(eval_ckpt_path):\n",
    "\n",
    "    state = torch.load(eval_ckpt_path)\n",
    "    start_batch = state[\"last_batch\"] + 1\n",
    "    accumulated_loss = state[\"accumulated_loss\"]\n",
    "    total_tokens = state[\"total_tokens\"]\n",
    "    print(f\"Resuming test-eval from batch {start_batch} (saved on disk).\")\n",
    "else:\n",
    "\n",
    "    start_batch = 0\n",
    "    accumulated_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    print(\"Starting test-eval from batch 0.\")\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(test_loader):\n",
    "        if batch_idx < start_batch:\n",
    "            continue\n",
    "\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels\n",
    "        )\n",
    "        loss = outputs.loss.detach().cpu().item()\n",
    "\n",
    "\n",
    "        nonpad_tokens = (labels != tokenizer.pad_token_id).sum().item()\n",
    "\n",
    "\n",
    "        accumulated_loss += loss * nonpad_tokens\n",
    "        total_tokens += nonpad_tokens\n",
    "\n",
    "\n",
    "        if (batch_idx + 1) % save_every_n_batches == 0:\n",
    "            state = {\n",
    "                \"last_batch\": batch_idx,\n",
    "                \"accumulated_loss\": accumulated_loss,\n",
    "                \"total_tokens\": total_tokens\n",
    "            }\n",
    "            torch.save(state, eval_ckpt_path)\n",
    "            print(f\" Saved eval state at batch {batch_idx} → tokens={total_tokens}\")\n",
    "\n",
    "    final_avg_loss = accumulated_loss / total_tokens\n",
    "    test_perplexity = torch.exp(torch.tensor(final_avg_loss)).item()\n",
    "    os.remove(eval_ckpt_path)\n",
    "\n",
    "    print(f\"\\n→ Test complete. Avg. token-loss = {final_avg_loss:.4f}\")\n",
    "    print(f\"→ Test Perplexity = {test_perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GUhRkaTENbwF",
    "outputId": "4de6d9bc-4c49-46e1-b5a7-bd6bc2e40565"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/usr/local/lib/python3.11/dist-packages/peft/peft_model.py:1926: UserWarning: Position ids are not supported for parameter efficient tuning. Ignoring position ids.\n",
      "  warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Latency (per prompt, 128 new tokens): 7.3562 seconds\n"
     ]
    }
   ],
   "source": [
    "# 2. LATENCY\n",
    "\n",
    "import time\n",
    "\n",
    "n_samples = min(50, len(test_prompts))\n",
    "max_new_tokens = 128\n",
    "batch_size = 4\n",
    "\n",
    "\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "latencies = []\n",
    "model.eval()\n",
    "device = model.device\n",
    "\n",
    "\n",
    "dummy_input = tokenizer(\"Hello\", return_tensors=\"pt\").to(device)\n",
    "_ = model.generate(\n",
    "    **dummy_input,\n",
    "    max_new_tokens=10,\n",
    "    do_sample=False,\n",
    "    use_cache=True,\n",
    "    return_dict_in_generate=False\n",
    ")\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "\n",
    "for i in range(0, n_samples, batch_size):\n",
    "    batch_prompts = test_prompts[i : i + batch_size]\n",
    "    inputs = tokenizer(\n",
    "        batch_prompts,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True\n",
    "    ).to(device)\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    _ = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "        return_dict_in_generate=False\n",
    "    )\n",
    "    torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "\n",
    "\n",
    "    elapsed = end - start\n",
    "    latencies.append(elapsed / len(batch_prompts))\n",
    "\n",
    "avg_latency = sum(latencies) / len(latencies)\n",
    "print(f\"Average Latency (per prompt, {max_new_tokens} new tokens): {avg_latency:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R0Iyt-kRRx0K",
    "outputId": "48f6364b-be83-4d08-956c-687943fa8828"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size on Disk : 9.2 MB\n"
     ]
    }
   ],
   "source": [
    "# 3. MODEL SIZE ON DISK\n",
    "\n",
    "def folder_size_in_mb(path: str) -> float:\n",
    "    total_bytes = 0\n",
    "    for root, _, files in os.walk(path):\n",
    "        for fname in files:\n",
    "            fp = os.path.join(root, fname)\n",
    "            total_bytes += os.path.getsize(fp)\n",
    "    return total_bytes / (1024 ** 2)\n",
    "\n",
    "model_size_mb = folder_size_in_mb(\"/content/drive/MyDrive/medical_Adapter\")\n",
    "print(f\"Model Size on Disk : {model_size_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "4fOrjJ-WUNdq"
   },
   "outputs": [],
   "source": [
    "# Generation for ROUGE calculation\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def optimized_medical_generation(\n",
    "    model, tokenizer, prompts, references,\n",
    "    checkpoint_path, batch_size=8, max_length=512\n",
    "):\n",
    "\n",
    "\n",
    "    if os.path.isdir(checkpoint_path):\n",
    "        os.makedirs(checkpoint_path, exist_ok=True)\n",
    "        state_file = os.path.join(checkpoint_path, \"generation_resume.json\")\n",
    "    else:\n",
    "        state_file = checkpoint_path\n",
    "\n",
    "\n",
    "    if os.path.exists(state_file):\n",
    "        with open(state_file, 'r') as f:\n",
    "            state = json.load(f)\n",
    "        completed_indices = state.get('completed_indices', [])\n",
    "        predictions = state.get('predictions', [])\n",
    "        print(f\"Resuming from {len(completed_indices)} completed samples\")\n",
    "    else:\n",
    "        completed_indices = []\n",
    "        predictions = []\n",
    "        state = {'completed_indices': completed_indices, 'predictions': predictions}\n",
    "\n",
    "    completed_set = set(completed_indices)\n",
    "    remaining_indices = [i for i in range(len(prompts)) if i not in completed_set]\n",
    "\n",
    "    if not remaining_indices:\n",
    "        print(\"All samples already processed!\")\n",
    "        return predictions, [references[i] for i in completed_indices]\n",
    "\n",
    "    model.eval()\n",
    "    device = model.device\n",
    "\n",
    "\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        emb = model.get_input_embeddings().weight\n",
    "        emb[tokenizer.pad_token_id].zero_()\n",
    "\n",
    "\n",
    "    for batch_start in tqdm(range(0, len(remaining_indices), batch_size),\n",
    "                            desc=\"Medical Generation\"):\n",
    "        batch_end     = min(batch_start + batch_size, len(remaining_indices))\n",
    "        batch_indices = remaining_indices[batch_start:batch_end]\n",
    "        batch_prompts = [prompts[i] for i in batch_indices]\n",
    "\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            [f\"MEDICAL PROMPT: {p}\" for p in batch_prompts],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_attention_mask=True,\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids      = inputs[\"input_ids\"],\n",
    "                attention_mask = inputs[\"attention_mask\"],\n",
    "                pad_token_id   = tokenizer.pad_token_id,\n",
    "                max_new_tokens = 256,\n",
    "                do_sample      = False,\n",
    "                num_beams      = 1,\n",
    "                use_cache      = True,\n",
    "            )\n",
    "\n",
    "        decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "        batch_preds = []\n",
    "        for prompt, text in zip(batch_prompts, decoded):\n",
    "            prefix = f\"MEDICAL PROMPT: {prompt}\"\n",
    "            if text.startswith(prefix):\n",
    "                gen_text = text[len(prefix):].strip()\n",
    "            else:\n",
    "                gen_text = text.strip()\n",
    "            batch_preds.append(gen_text)\n",
    "\n",
    "        predictions.extend(batch_preds)\n",
    "        completed_indices.extend(batch_indices)\n",
    "\n",
    "\n",
    "        with open(state_file, 'w') as f:\n",
    "            json.dump({\n",
    "                'completed_indices': completed_indices,\n",
    "                'predictions': predictions\n",
    "            }, f)\n",
    "\n",
    "    return predictions, [references[i] for i in completed_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "y-hGTDQ6VAm1"
   },
   "outputs": [],
   "source": [
    "latest_checkpoint = \"/content/drive/MyDrive/medical_Adapter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RvGZeoluUXck",
    "outputId": "0895be30-af16-42dd-f316-2477fef0f233"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Medical Generation: 100%|██████████| 84/84 [1:29:12<00:00, 63.72s/it]\n"
     ]
    }
   ],
   "source": [
    "predictions, processed_refs = optimized_medical_generation(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    test_prompts,\n",
    "    test_references,\n",
    "    latest_checkpoint,\n",
    "    batch_size=12,\n",
    "    max_length=120\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HArGihP4qDg3",
    "outputId": "10fa4c7b-9682-490a-af56-5fc1623ca507"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install evaluate --q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "b28f11d771864d9483ab7a9ba90610e9",
      "174f5afa1b9a48fb87b1c4b0f8d3c28d",
      "636f556d98eb4e189a28218b2c1f5b79",
      "cd1a33afe1fa4af297884a5a2e8dc945",
      "b85a887fca054a34b2c1bac2e22d32cf",
      "07ae2050735b4e3382e65acc659855b7",
      "d80c5f1bb3e14e8f965e8a47f9829efa",
      "edaee832ece14cf79cfa13b981fe3005",
      "1dbf846c2e98449b8fabfeb4e035db14",
      "1be8bbda547e4a0da8b70e4844082b91",
      "480d3437d355468c85be7d6cff691386"
     ]
    },
    "id": "TzP3LmHpUarX",
    "outputId": "fc77ebc2-6c57-4325-b1ed-61ccbb4d9d68"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b28f11d771864d9483ab7a9ba90610e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from evaluate import load\n",
    "rouge = load(\"rouge\")\n",
    "results = rouge.compute(\n",
    "        predictions=predictions,\n",
    "        references=processed_refs,\n",
    "        use_stemmer=True,\n",
    "        use_aggregator=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZntNHH2rVTwl",
    "outputId": "3e1f5193-20b4-4653-8583-c986d14cecbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Medical ROUGE Scores:\n",
      "ROUGE-1: 0.1556\n",
      "ROUGE-2: 0.0226\n",
      "ROUGE-L: 0.0956\n",
      "ROUGE-Lsum: 0.0988\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nMedical ROUGE Scores:\")\n",
    "print(f\"ROUGE-1: {results['rouge1']:.4f}\")\n",
    "print(f\"ROUGE-2: {results['rouge2']:.4f}\")\n",
    "print(f\"ROUGE-L: {results['rougeL']:.4f}\")\n",
    "print(f\"ROUGE-Lsum: {results['rougeLsum']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A_PaKXkOqgKe"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7736069,
     "sourceId": 12276062,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
